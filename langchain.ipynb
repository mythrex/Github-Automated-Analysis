{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "import glob\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.refine import RefineDocumentsChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = PromptTemplate(template=map_template_string, input_variables=[\"code\"])\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "# llm_chain.run(code=code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give the following code information, for the following and access the code quality in terms of following points.\\n    Answer the following if possible with a score 0-10.\\n        1. lines of code\\n        2. cyclomatic complexity\\n        3. nesting depth\\n        4. code duplication\\n        5. code coupling\\n        6. Code readability\\n        7. Code maintanibility\\n        8. Proper documentation\\n        9. Proper function doc strings\\n        10. Proper maintained readme.\\n\\n    The code is given in following format.\\n    Format:\\n        filename\\n        ##########\\n        code\\n\\n    Code:\\n        {code}\\n    '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Give the following code information, for the following and access the code quality in terms of following points.\n",
    "    Answer the following if possible with a score 0-10.\n",
    "        1. lines of code\n",
    "        2. cyclomatic complexity\n",
    "        3. nesting depth\n",
    "        4. code duplication\n",
    "        5. code coupling\n",
    "        6. Code readability\n",
    "        7. Code maintanibility\n",
    "        8. Proper documentation\n",
    "        9. Proper function doc strings\n",
    "        10. Proper maintained readme.\n",
    "\n",
    "    The code is given in following format.\n",
    "    Format:\n",
    "        filename\n",
    "        ##########\n",
    "        code\n",
    "\n",
    "    Code:\n",
    "        {code}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "EXTENSION_SET = {\n",
    "    \".py\",  # Python\n",
    "    \".java\",  # Java\n",
    "    \".js\",  # JavaScript\n",
    "    \".cpp\",  # C++\n",
    "    \".c\",  # C\n",
    "    \".html\",  # HTML\n",
    "    \".css\",  # CSS\n",
    "    \".php\",  # PHP\n",
    "    \".rb\",  # Ruby\n",
    "    \".swift\",  # Swift\n",
    "    \".go\",  # Go\n",
    "    \".ts\",  # TypeScript\n",
    "    \".sh\",  # Shell script\n",
    "    \".pl\",  # Perl\n",
    "    \".r\",  # R\n",
    "    \".scala\",  # Scala\n",
    "    \".lua\",  # Lua\n",
    "    \".md\",  # Markdown\n",
    "    \".json\",  # JSON\n",
    "    \".xml\",  # XML\n",
    "    \".yaml\",  # YAML\n",
    "    \".sql\",  # SQL\n",
    "    \".h\",  # Header file\n",
    "    \".hpp\",  # C++ header file\n",
    "    \".cs\",  # C#\n",
    "    \".vb\",  # Visual Basic\n",
    "    \".asm\",  # Assembly\n",
    "    \".dockerfile\",  # Dockerfile\n",
    "    \".yml\",  # YAML (alternative extension)\n",
    "    \".kt\",  # Kotlin\n",
    "    \".jl\",  # Julia\n",
    "    \".groovy\",  # Groovy\n",
    "    \".pl\",  # Prolog\n",
    "    \".ps1\",  # PowerShell\n",
    "    \".tex\",  # LaTeX\n",
    "    \".matlab\",  # MATLAB\n",
    "    \".m\",  # MATLAB (alternative extension)\n",
    "    \".dart\",  # Dart\n",
    "    \".bash\",  # Bash script\n",
    "    \".jsx\",  # JSX (JavaScript extension)\n",
    "    \".tsx\",  # TSX (TypeScript extension)\n",
    "    \".cfg\",  # Configuration file\n",
    "    \".ini\",  # INI file\n",
    "    \".md\",  # Markdown file\n",
    "}\n",
    "\n",
    "\n",
    "class LLMCodeAnalyser:\n",
    "    map_template_string = \"\"\"Give the following code information, for the following and access the code quality in terms of following points.\n",
    "    Answer the following if possible with a score 0-10.\n",
    "        1. lines of code\n",
    "        2. cyclomatic complexity\n",
    "        3. nesting depth\n",
    "        4. code duplication\n",
    "        5. code coupling\n",
    "        6. Code readability\n",
    "        7. Code maintanibility\n",
    "        8. Proper documentation\n",
    "        9. Proper function doc strings\n",
    "        10. Proper maintained readme.\n",
    "\n",
    "    Code:\n",
    "        {code}\n",
    "    \"\"\"\n",
    "\n",
    "    reduce_template_string = \"\"\"Given the information about the code quality, \n",
    "    Aggregate the results below and convert it to python dict\n",
    "        {code_description}\n",
    "        Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.llm = OpenAI()\n",
    "\n",
    "    def _convert_notebook_to_code_string(notebook_path: str):\n",
    "        # Open the Jupyter Notebook file\n",
    "        with open(notebook_path, \"r\") as f:\n",
    "            notebook_content = json.load(f)\n",
    "\n",
    "        code_cells = []\n",
    "        for cell in notebook_content[\"cells\"]:\n",
    "            if cell[\"cell_type\"] == \"code\":\n",
    "                code = \"\".join(cell[\"source\"])\n",
    "                code_cells.append(code)\n",
    "            elif cell[\"cell_type\"] == \"mardown\":\n",
    "                comment = \"\".join(cell[\"source\"])\n",
    "                code_cells.append(\"'''\" + comment + \"'''\")\n",
    "\n",
    "        code_string = \"\\n\".join(code_cells)\n",
    "        return code_string\n",
    "\n",
    "    def _load_text_files(self, file):\n",
    "        # handle notebooks\n",
    "        extension = \".\" + file.split(\".\")[-1]\n",
    "        if extension == \".ipynb\":\n",
    "            code = self._convert_notebook_to_code_string(file)\n",
    "            code = f\"------------------------------------\\n{file}\\n{code}\"\n",
    "            return code\n",
    "        elif extension in EXTENSION_SET:\n",
    "            try:\n",
    "                f = open(file, \"r\")\n",
    "                code = f.read()\n",
    "                code = f\"------------------------------------\\n{file}\\n{code}\"\n",
    "                return code\n",
    "            except:\n",
    "                return None\n",
    "    \n",
    "    def _get_file_paths(self, directory):\n",
    "        file_paths = []\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_paths.append(file_path)\n",
    "        return file_paths\n",
    "\n",
    "    def get_code(self, repo_path: str, sampling_rate: float = 0.25):\n",
    "        files = self._get_file_paths(repo_path)\n",
    "        files = sorted(files, key=lambda x: len(x.split(\"/\")))\n",
    "        min_level = min(map(lambda x: len(x.split(\"/\")), files))\n",
    "        files_at_level0 = list(filter(lambda x: len(x.split(\"/\")) == min_level, files))\n",
    "        random_sampled_files = random.sample(\n",
    "            list(filter(lambda x: (len(x.split(\"/\")) != min_level) and '.' in x, files)),\n",
    "            k=min(10, int(sampling_rate * len(files))),\n",
    "        )\n",
    "        files = files_at_level0 + random_sampled_files\n",
    "        files = list(\n",
    "            filter(\n",
    "                lambda x: x is not None and len(x) > 0,\n",
    "                map(lambda x: self._load_text_files(x), files),\n",
    "            )\n",
    "        )\n",
    "        files = \"\".join(files)\n",
    "        return files\n",
    "\n",
    "    def analyse_repo_gpt(self, repo_path: str) -> dict:\n",
    "        codebase = self.get_code(repo_path)\n",
    "        map_prompt = PromptTemplate(\n",
    "            input_variables=[\"code\"], template=self.map_template_string\n",
    "        )\n",
    "        map_llm_chain = LLMChain(llm=self.llm, prompt=map_prompt)\n",
    "        reduce_prompt = PromptTemplate(\n",
    "            input_variables=[\"code_description\"], template=self.reduce_template_string\n",
    "        )\n",
    "        reduce_llm_chain = LLMChain(llm=self.llm, prompt=reduce_prompt)\n",
    "        generative_result_reduce_chain = StuffDocumentsChain(\n",
    "            llm_chain=reduce_llm_chain,\n",
    "            document_variable_name=\"code_description\",\n",
    "        )\n",
    "        combine_documents = MapReduceDocumentsChain(\n",
    "            llm_chain=map_llm_chain,\n",
    "            combine_document_chain=generative_result_reduce_chain,\n",
    "            document_variable_name=\"code\",\n",
    "        )\n",
    "        map_reduce = MapReduceChain(\n",
    "            combine_documents_chain=combine_documents,\n",
    "            text_splitter=RecursiveCharacterTextSplitter(\n",
    "                chunk_size=2000,\n",
    "                chunk_overlap=10,\n",
    "                length_function=len,\n",
    "            ),\n",
    "        )\n",
    "        result = map_reduce.run(input_text=codebase, verbose=True)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_analyser = LLMCodeAnalyser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = 'repos/darts'\n",
    "codebase = code_analyser.get_code(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m code_analyser\u001b[39m.\u001b[39;49manalyse_repo_gpt(repo_path)\n",
      "Cell \u001b[0;32mIn[5], line 153\u001b[0m, in \u001b[0;36mLLMCodeAnalyser.analyse_repo_gpt\u001b[0;34m(self, repo_path)\u001b[0m\n\u001b[1;32m    140\u001b[0m combine_documents \u001b[39m=\u001b[39m MapReduceDocumentsChain(\n\u001b[1;32m    141\u001b[0m     llm_chain\u001b[39m=\u001b[39mmap_llm_chain,\n\u001b[1;32m    142\u001b[0m     combine_document_chain\u001b[39m=\u001b[39mgenerative_result_reduce_chain,\n\u001b[1;32m    143\u001b[0m     document_variable_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    144\u001b[0m )\n\u001b[1;32m    145\u001b[0m map_reduce \u001b[39m=\u001b[39m MapReduceChain(\n\u001b[1;32m    146\u001b[0m     combine_documents_chain\u001b[39m=\u001b[39mcombine_documents,\n\u001b[1;32m    147\u001b[0m     text_splitter\u001b[39m=\u001b[39mRecursiveCharacterTextSplitter(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m     ),\n\u001b[1;32m    152\u001b[0m )\n\u001b[0;32m--> 153\u001b[0m result \u001b[39m=\u001b[39m map_reduce\u001b[39m.\u001b[39;49mrun(input_text\u001b[39m=\u001b[39;49mcodebase, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:270\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:149\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    150\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    151\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    152\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    153\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:143\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    137\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    138\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    139\u001b[0m     inputs,\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    144\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    145\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/mapreduce.py:101\u001b[0m, in \u001b[0;36mMapReduceChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     96\u001b[0m docs \u001b[39m=\u001b[39m [Document(page_content\u001b[39m=\u001b[39mtext) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]\n\u001b[1;32m     97\u001b[0m _inputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {\n\u001b[1;32m     98\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs,\n\u001b[1;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_documents_chain\u001b[39m.\u001b[39minput_key: docs,\n\u001b[1;32m    100\u001b[0m }\n\u001b[0;32m--> 101\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_documents_chain\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    102\u001b[0m     _inputs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child()\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: outputs}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:267\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    270\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:149\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    150\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    151\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    152\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    153\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:143\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    137\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    138\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    139\u001b[0m     inputs,\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    144\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    145\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:84\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     83\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m---> 84\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m     85\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:144\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcombine_docs\u001b[39m(\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    134\u001b[0m     docs: List[Document],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    138\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[1;32m    139\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m \u001b[39m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    145\u001b[0m         \u001b[39m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[1;32m    146\u001b[0m         [{\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocument_variable_name: d\u001b[39m.\u001b[39;49mpage_content, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs} \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m docs],\n\u001b[1;32m    147\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    148\u001b[0m     )\n\u001b[1;32m    149\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_results(\n\u001b[1;32m    150\u001b[0m         results, docs, token_max, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    151\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/llm.py:162\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    161\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    163\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)\n\u001b[1;32m    164\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end({\u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: outputs})\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/llm.py:159\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39minput_list\u001b[39m\u001b[39m\"\u001b[39m: input_list},\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    160\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    161\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/llm.py:83\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]],\n\u001b[1;32m     80\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     81\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m     82\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     84\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[1;32m     85\u001b[0m         prompts, stop, callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/llm.py:106\u001b[0m, in \u001b[0;36mLLMChain.prep_prompts\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Prepare prompts from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m stop \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m input_list[\u001b[39m0\u001b[39;49m]:\n\u001b[1;32m    107\u001b[0m     stop \u001b[39m=\u001b[39m input_list[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    108\u001b[0m prompts \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "code_analyser.analyse_repo_gpt(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 2049 tokens, however you requested 3091 tokens (2835 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m map_reduce\u001b[39m.\u001b[39;49mrun(input_text\u001b[39m=\u001b[39;49mcodebase)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:270\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:149\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    150\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    151\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    152\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    153\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:143\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    137\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    138\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    139\u001b[0m     inputs,\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    144\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    145\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/mapreduce.py:101\u001b[0m, in \u001b[0;36mMapReduceChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     96\u001b[0m docs \u001b[39m=\u001b[39m [Document(page_content\u001b[39m=\u001b[39mtext) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts]\n\u001b[1;32m     97\u001b[0m _inputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {\n\u001b[1;32m     98\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs,\n\u001b[1;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_documents_chain\u001b[39m.\u001b[39minput_key: docs,\n\u001b[1;32m    100\u001b[0m }\n\u001b[0;32m--> 101\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_documents_chain\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    102\u001b[0m     _inputs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child()\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: outputs}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:267\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    270\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:149\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    150\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    151\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    152\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    153\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:143\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    137\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    138\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    139\u001b[0m     inputs,\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    144\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    145\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:84\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     83\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m---> 84\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m     85\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:149\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m \u001b[39mCombine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mThis reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mapply(\n\u001b[1;32m    145\u001b[0m     \u001b[39m# FYI - this is parallelized and so it is fast.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     [{\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_variable_name: d\u001b[39m.\u001b[39mpage_content, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs} \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m docs],\n\u001b[1;32m    147\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_results(\n\u001b[1;32m    150\u001b[0m     results, docs, token_max, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    151\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:216\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain._process_results\u001b[0;34m(self, results, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_process_results\u001b[39m(\n\u001b[1;32m    209\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    210\u001b[0m     results: List[Dict],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[0;32m--> 216\u001b[0m     result_docs, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_results_common(\n\u001b[1;32m    217\u001b[0m         results, docs, token_max, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_document_chain\u001b[39m.\u001b[39mrun(\n\u001b[1;32m    220\u001b[0m         input_documents\u001b[39m=\u001b[39mresult_docs, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    221\u001b[0m     )\n\u001b[1;32m    222\u001b[0m     \u001b[39mreturn\u001b[39;00m output, extra_return_dict\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:198\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain._process_results_common\u001b[0;34m(self, results, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m result_docs \u001b[39m=\u001b[39m []\n\u001b[1;32m    197\u001b[0m \u001b[39mfor\u001b[39;00m docs \u001b[39min\u001b[39;00m new_result_doc_list:\n\u001b[0;32m--> 198\u001b[0m     new_doc \u001b[39m=\u001b[39m _collapse_docs(docs, _collapse_docs_func, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    199\u001b[0m     result_docs\u001b[39m.\u001b[39mappend(new_doc)\n\u001b[1;32m    200\u001b[0m num_tokens \u001b[39m=\u001b[39m length_func(result_docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:52\u001b[0m, in \u001b[0;36m_collapse_docs\u001b[0;34m(docs, combine_document_func, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_collapse_docs\u001b[39m(\n\u001b[1;32m     48\u001b[0m     docs: List[Document],\n\u001b[1;32m     49\u001b[0m     combine_document_func: CombineDocsProtocol,\n\u001b[1;32m     50\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     51\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Document:\n\u001b[0;32m---> 52\u001b[0m     result \u001b[39m=\u001b[39m combine_document_func(docs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     53\u001b[0m     combined_metadata \u001b[39m=\u001b[39m {k: \u001b[39mstr\u001b[39m(v) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m docs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     54\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs[\u001b[39m1\u001b[39m:]:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:188\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain._process_results_common.<locals>._collapse_docs_func\u001b[0;34m(docs, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_collapse_docs_func\u001b[39m(docs: List[Document], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m--> 188\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_collapse_chain\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    189\u001b[0m         input_documents\u001b[39m=\u001b[39;49mdocs, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    190\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:270\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:149\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    150\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    151\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    152\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    153\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:143\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    137\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    138\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    139\u001b[0m     inputs,\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    144\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    145\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:84\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     83\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m---> 84\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m     85\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py:87\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_inputs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     86\u001b[0m \u001b[39m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mpredict(callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs), {}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/llm.py:218\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \n\u001b[1;32m    206\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:149\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    150\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    151\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    152\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    153\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/base.py:143\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    137\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    138\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    139\u001b[0m     inputs,\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    144\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    145\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/llm.py:74\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     70\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     71\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     72\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 74\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/llm.py:84\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m---> 84\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m     85\u001b[0m     prompts, stop, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m     86\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/llms/base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    134\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    138\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    139\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/llms/base.py:206\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    205\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    207\u001b[0m run_manager\u001b[39m.\u001b[39mon_llm_end(output)\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m run_manager:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/llms/base.py:198\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    194\u001b[0m     dumpd(\u001b[39mself\u001b[39m), prompts, invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[1;32m    195\u001b[0m )\n\u001b[1;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 198\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    199\u001b[0m             prompts, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    200\u001b[0m         )\n\u001b[1;32m    201\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    202\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    205\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/llms/openai.py:326\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    325\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 326\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\u001b[39mself\u001b[39;49m, prompt\u001b[39m=\u001b[39;49m_prompts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    327\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstreaming:\n\u001b[1;32m    329\u001b[0m     \u001b[39m# Can't update token usage if streaming\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/llms/openai.py:106\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 106\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tenacity/__init__.py:326\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tenacity/__init__.py:406\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    405\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    407\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    408\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tenacity/__init__.py:351\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    349\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m retry_state\u001b[39m.\u001b[39moutcome\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(retry_state\u001b[39m.\u001b[39moutcome\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state\u001b[39m=\u001b[39mretry_state)):\n\u001b[0;32m--> 351\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    353\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tenacity/__init__.py:409\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    408\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    410\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    411\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/llms/openai.py:104\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 2049 tokens, however you requested 3091 tokens (2835 in your prompt; 256 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "result = map_reduce.run(\n",
    "    input_text=codebase\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'initial_llm_chain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 33\u001b[0m\n\u001b[1;32m     26\u001b[0m reduce_llm_chain \u001b[39m=\u001b[39m LLMChain(llm\u001b[39m=\u001b[39mllm, prompt\u001b[39m=\u001b[39mREDUCE_PROMPT)\n\u001b[1;32m     28\u001b[0m generative_result_reduce_chain \u001b[39m=\u001b[39m StuffDocumentsChain(\n\u001b[1;32m     29\u001b[0m     llm_chain\u001b[39m=\u001b[39mreduce_llm_chain,\n\u001b[1;32m     30\u001b[0m     document_variable_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcode_description\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m combine_documents \u001b[39m=\u001b[39m RefineDocumentsChain(\n\u001b[1;32m     34\u001b[0m     llm_chain\u001b[39m=\u001b[39;49mmap_llm_chain,\n\u001b[1;32m     35\u001b[0m     combine_document_chain\u001b[39m=\u001b[39;49mgenerative_result_reduce_chain,\n\u001b[1;32m     36\u001b[0m     document_variable_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcode\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m map_reduce \u001b[39m=\u001b[39m MapReduceChain(\n\u001b[1;32m     40\u001b[0m     combine_documents_chain\u001b[39m=\u001b[39mcombine_documents,\n\u001b[1;32m     41\u001b[0m     text_splitter\u001b[39m=\u001b[39mRecursiveCharacterTextSplitter(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m             ),\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     48\u001b[0m code \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef bubblesort(list):\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[39m   for iter_num in range(len(list)-1,0,-1):\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m   return input_list\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[39m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/load/serializable.py:64\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pydantic/main.py:1050\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/langchain/chains/combine_documents/refine.py:81\u001b[0m, in \u001b[0;36mRefineDocumentsChain.get_default_document_variable_name\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     77\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mdocument_variable_name must be provided if there are \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmultiple llm_chain input_variables\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         )\n\u001b[1;32m     80\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     llm_chain_variables \u001b[39m=\u001b[39m values[\u001b[39m\"\u001b[39;49m\u001b[39minitial_llm_chain\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mprompt\u001b[39m.\u001b[39minput_variables\n\u001b[1;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m values[\u001b[39m\"\u001b[39m\u001b[39mdocument_variable_name\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m llm_chain_variables:\n\u001b[1;32m     83\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     84\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdocument_variable_name \u001b[39m\u001b[39m{\u001b[39;00mvalues[\u001b[39m'\u001b[39m\u001b[39mdocument_variable_name\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m was \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnot found in llm_chain input_variables: \u001b[39m\u001b[39m{\u001b[39;00mllm_chain_variables\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m         )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'initial_llm_chain'"
     ]
    }
   ],
   "source": [
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.combine_documents.refine import RefineDocumentsChain\n",
    "\n",
    "map_template_string = \"\"\"Given the following python code information, generate a description that explains what the code does and also mention the time complexity.\n",
    "Code:\n",
    "{code}\n",
    "\n",
    "Return the the description in the following format:\n",
    "name of the function: description of the function\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "reduce_template_string = \"\"\"Given the following python function names and descriptions, answer the following question\n",
    "{code_description}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "MAP_PROMPT = PromptTemplate(input_variables=[\"code\"], template=map_template_string)\n",
    "REDUCE_PROMPT = PromptTemplate(input_variables=[\"code_description\", \"question\"], template=reduce_template_string)\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "map_llm_chain = LLMChain(llm=llm, prompt=MAP_PROMPT)\n",
    "reduce_llm_chain = LLMChain(llm=llm, prompt=REDUCE_PROMPT)\n",
    "\n",
    "generative_result_reduce_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_llm_chain,\n",
    "    document_variable_name=\"code_description\",\n",
    ")\n",
    "\n",
    "combine_documents = RefineDocumentsChain(\n",
    "    llm_chain=map_llm_chain,\n",
    "    combine_document_chain=generative_result_reduce_chain,\n",
    "    document_variable_name=\"code\",\n",
    ")\n",
    "\n",
    "map_reduce = MapReduceChain(\n",
    "    combine_documents_chain=combine_documents,\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "                chunk_size=2000,\n",
    "                chunk_overlap=10,\n",
    "                length_function=len,\n",
    "            ),\n",
    ")\n",
    "\n",
    "code = \"\"\"\n",
    "def bubblesort(list):\n",
    "   for iter_num in range(len(list)-1,0,-1):\n",
    "      for idx in range(iter_num):\n",
    "         if list[idx]>list[idx+1]:\n",
    "            temp = list[idx]\n",
    "            list[idx] = list[idx+1]\n",
    "            list[idx+1] = temp\n",
    "    return list\n",
    "##\n",
    "def insertion_sort(InputList):\n",
    "   for i in range(1, len(InputList)):\n",
    "      j = i-1\n",
    "      nxt_element = InputList[i]\n",
    "   while (InputList[j] > nxt_element) and (j >= 0):\n",
    "      InputList[j+1] = InputList[j]\n",
    "      j=j-1\n",
    "   InputList[j+1] = nxt_element\n",
    "   return InputList\n",
    "##\n",
    "def shellSort(input_list):\n",
    "   gap = len(input_list) // 2\n",
    "   while gap > 0:\n",
    "      for i in range(gap, len(input_list)):\n",
    "         temp = input_list[i]\n",
    "         j = i\n",
    "   while j >= gap and input_list[j - gap] > temp:\n",
    "      input_list[j] = input_list[j - gap]\n",
    "      j = j-gap\n",
    "      input_list[j] = temp\n",
    "   gap = gap//2\n",
    "   return input_list\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'map_reduce' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m map_reduce\u001b[39m.\u001b[39mrun(input_text\u001b[39m=\u001b[39mcodebase, question\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWhat are the function names?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'map_reduce' is not defined"
     ]
    }
   ],
   "source": [
    "map_reduce.run(input_text=codebase, question=\"What are the function names?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Differentiable Architecture Search\\nCode accompanying the paper\\n> [DARTS: Differentiable Architecture Search](https://arxiv.org/abs/1806.09055)\\\\\\n> Hanxiao Liu, Karen Simonyan, Yiming Yang.\\\\\\n> _arXiv:1806.09055_.\\n\\n<p align=\"center\">\\n  <img src=\"img/darts.png\" alt=\"darts\" width=\"48%\">\\n</p>\\nThe algorithm is based on continuous relaxation and gradient descent in the architecture space. It is able to efficiently design high-performance convolutional architectures for image classification (on CIFAR-10 and ImageNet) and recurrent architectures for language modeling (on Penn Treebank and WikiText-2). Only a single GPU is required.\\n\\n## Requirements\\n```\\nPython >= 3.5.5, PyTorch == 0.3.1, torchvision == 0.2.0\\n```\\nNOTE: PyTorch 0.4 is not supported at this moment and would lead to OOM.\\n\\n## Datasets\\nInstructions for acquiring PTB and WT2 can be found [here](https://github.com/salesforce/awd-lstm-lm). While CIFAR-10 can be automatically downloaded by torchvision, ImageNet needs to be manually downloaded (preferably to a SSD) following the instructions [here](https://github.com/pytorch/examples/tree/master/imagenet).\\n\\n## Pretrained models\\nThe easist way to get started is to evaluate our pretrained DARTS models.\\n\\n**CIFAR-10** ([cifar10_model.pt](https://drive.google.com/file/d/1Y13i4zKGKgjtWBdC0HWLavjO7wvEiGOc/view?usp=sharing))\\n```\\ncd cnn && python test.py --auxiliary --model_path cifar10_model.pt\\n```\\n* Expected result: 2.63% test error rate with 3.3M model params.\\n\\n**PTB** ([ptb_model.pt](https://drive.google.com/file/d/1Mt_o6fZOlG-VDF3Q5ModgnAJ9W6f_av2/view?usp=sharing))\\n```\\ncd rnn && python test.py --model_path ptb_model.pt\\n```\\n* Expected result: 55.68 test perplexity with 23M model params.\\n\\n**ImageNet** ([imagenet_model.pt](https://drive.google.com/file/d/1AKr6Y_PoYj7j0Upggyzc26W0RVdg4CVX/view?usp=sharing))\\n```\\ncd cnn && python test_imagenet.py --auxiliary --model_path imagenet_model.pt\\n```\\n* Expected result: 26.7% top-1 error and 8.7% top-5 error with 4.7M model params.\\n\\n## Architecture search (using small proxy models)\\nTo carry out architecture search using 2nd-order approximation, run\\n```\\ncd cnn && python train_search.py --unrolled     # for conv cells on CIFAR-10\\ncd rnn && python train_search.py --unrolled     # for recurrent cells on PTB\\n```\\nNote the _validation performance in this step does not indicate the final performance of the architecture_. One must train the obtained genotype/architecture from scratch using full-sized models, as described in the next section.\\n\\nAlso be aware that different runs would end up with different local minimum. To get the best result, it is crucial to repeat the search process with different seeds and select the best cell(s) based on validation performance (obtained by training the derived cell from scratch for a small number of epochs). Please refer to fig. 3 and sect. 3.2 in our arXiv paper.\\n\\n<p align=\"center\">\\n<img src=\"img/progress_convolutional_normal.gif\" alt=\"progress_convolutional_normal\" width=\"29%\">\\n<img src=\"img/progress_convolutional_reduce.gif\" alt=\"progress_convolutional_reduce\" width=\"35%\">\\n<img src=\"img/progress_recurrent.gif\" alt=\"progress_recurrent\" width=\"33%\">\\n</p>\\n<p align=\"center\">\\nFigure: Snapshots of the most likely normal conv, reduction conv, and recurrent cells over time.\\n</p>\\n\\n## Architecture evaluation (using full-sized models)\\nTo evaluate our best cells by training from scratch, run\\n```\\ncd cnn && python train.py --auxiliary --cutout            # CIFAR-10\\ncd rnn && python train.py                                 # PTB\\ncd rnn && python train.py --data ../data/wikitext-2 \\\\     # WT2\\n            --dropouth 0.15 --emsize 700 --nhidlast 700 --nhid 700 --wdecay 5e-7\\ncd cnn && python train_imagenet.py --auxiliary            # ImageNet\\n```\\nCustomized architectures are supported through the `--arch` flag once specified in `genotypes.py`.\\n\\nThe CIFAR-10 result at the end of training is subject to variance due to the non-determinism of cuDNN back-prop kernels. _It would be misleading to report the result of only a single run_. By training our best cell from scratch, one should expect the average test error of 10 independent runs to fall in the range of 2.76 +/- 0.09% with high probability.\\n\\n<p align=\"center\">\\n<img src=\"img/cifar10.png\" alt=\"cifar10\" width=\"36%\">\\n<img src=\"img/imagenet.png\" alt=\"ptb\" width=\"29%\">\\n<img src=\"img/ptb.png\" alt=\"ptb\" width=\"30%\">\\n</p>\\n<p align=\"center\">\\nFigure: Expected learning curves on CIFAR-10 (4 runs), ImageNet and PTB.\\n</p>\\n\\n## Visualization\\nPackage [graphviz](https://graphviz.readthedocs.io/en/stable/index.html) is required to visualize the learned cells\\n```\\npython visualize.py DARTS\\n```\\nwhere `DARTS` can be replaced by any customized architectures in `genotypes.py`.\\n\\n## Citation\\nIf you use any part of this code in your research, please cite our [paper](https://arxiv.org/abs/1806.09055):\\n```\\n@article{liu2018darts,\\n  title={DARTS: Differentiable Architecture Search},\\n  author={Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},\\n  journal={arXiv preprint arXiv:1806.09055},\\n  year={2018}\\n}\\n```\\nimport os\\nimport sys\\nimport numpy as np\\nimport torch\\nimport utils\\nimport glob\\nimport random\\nimport logging\\nimport argparse\\nimport torch.nn as nn\\nimport genotypes\\nimport torch.utils\\nimport torchvision.datasets as dset\\nimport torchvision.transforms as transforms\\nimport torch.backends.cudnn as cudnn\\n\\nfrom torch.autograd import Variable\\nfrom model import NetworkImageNet as Network\\n\\n\\nparser = argparse.ArgumentParser(\"imagenet\")\\nparser.add_argument(\\'--data\\', type=str, default=\\'../data/imagenet/\\', help=\\'location of the data corpus\\')\\nparser.add_argument(\\'--batch_size\\', type=int, default=128, help=\\'batch size\\')\\nparser.add_argument(\\'--report_freq\\', type=float, default=100, help=\\'report frequency\\')\\nparser.add_argument(\\'--gpu\\', type=int, default=0, help=\\'gpu device id\\')\\nparser.add_argument(\\'--init_channels\\', type=int, default=48, help=\\'num of init channels\\')\\nparser.add_argument(\\'--layers\\', type=int, default=14, help=\\'total number of layers\\')\\nparser.add_argument(\\'--model_path\\', type=str, default=\\'EXP/model.pt\\', help=\\'path of pretrained model\\')\\nparser.add_argument(\\'--auxiliary\\', action=\\'store_true\\', default=False, help=\\'use auxiliary tower\\')\\nparser.add_argument(\\'--drop_path_prob\\', type=float, default=0, help=\\'drop path probability\\')\\nparser.add_argument(\\'--seed\\', type=int, default=0, help=\\'random seed\\')\\nparser.add_argument(\\'--arch\\', type=str, default=\\'DARTS\\', help=\\'which architecture to use\\')\\nargs = parser.parse_args()\\n\\nlog_format = \\'%(asctime)s %(message)s\\'\\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO,\\n    format=log_format, datefmt=\\'%m/%d %I:%M:%S %p\\')\\n\\nCLASSES = 1000\\n\\n\\ndef main():\\n  if not torch.cuda.is_available():\\n    logging.info(\\'no gpu device available\\')\\n    sys.exit(1)\\n\\n  np.random.seed(args.seed)\\n  torch.cuda.set_device(args.gpu)\\n  cudnn.benchmark = True\\n  torch.manual_seed(args.seed)\\n  cudnn.enabled=True\\n  torch.cuda.manual_seed(args.seed)\\n  logging.info(\\'gpu device = %d\\' % args.gpu)\\n  logging.info(\"args = %s\", args)\\n\\n  genotype = eval(\"genotypes.%s\" % args.arch)\\n  model = Network(args.init_channels, CLASSES, args.layers, args.auxiliary, genotype)\\n  model = model.cuda()\\n  model.load_state_dict(torch.load(args.model_path)[\\'state_dict\\'])\\n\\n  logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\\n\\n  criterion = nn.CrossEntropyLoss()\\n  criterion = criterion.cuda()\\n\\n  validdir = os.path.join(args.data, \\'val\\')\\n  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n  valid_data = dset.ImageFolder(\\n    validdir,\\n    transforms.Compose([\\n      transforms.Resize(256),\\n      transforms.CenterCrop(224),\\n      transforms.ToTensor(),\\n      normalize,\\n    ]))\\n\\n  valid_queue = torch.utils.data.DataLoader(\\n    valid_data, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=4)\\n\\n  model.drop_path_prob = args.drop_path_prob\\n  valid_acc_top1, valid_acc_top5, valid_obj = infer(valid_queue, model, criterion)\\n  logging.info(\\'valid_acc_top1 %f\\', valid_acc_top1)\\n  logging.info(\\'valid_acc_top5 %f\\', valid_acc_top5)\\n\\n\\ndef infer(valid_queue, model, criterion):\\n  objs = utils.AvgrageMeter()\\n  top1 = utils.AvgrageMeter()\\n  top5 = utils.AvgrageMeter()\\n  model.eval()\\n\\n  for step, (input, target) in enumerate(valid_queue):\\n    input = Variable(input, volatile=True).cuda()\\n    target = Variable(target, volatile=True).cuda(async=True)\\n\\n    logits, _ = model(input)\\n    loss = criterion(logits, target)\\n\\n    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\\n    n = input.size(0)\\n    objs.update(loss.data[0], n)\\n    top1.update(prec1.data[0], n)\\n    top5.update(prec5.data[0], n)\\n\\n    if step % args.report_freq == 0:\\n      logging.info(\\'valid %03d %e %f %f\\', step, objs.avg, top1.avg, top5.avg)\\n\\n  return top1.avg, top5.avg, objs.avg\\n\\n\\nif __name__ == \\'__main__\\':\\n  main() \\nimport sys\\nimport genotypes\\nfrom graphviz import Digraph\\n\\n\\ndef plot(genotype, filename):\\n  g = Digraph(\\n      format=\\'pdf\\',\\n      edge_attr=dict(fontsize=\\'20\\', fontname=\"times\"),\\n      node_attr=dict(style=\\'filled\\', shape=\\'rect\\', align=\\'center\\', fontsize=\\'20\\', height=\\'0.5\\', width=\\'0.5\\', penwidth=\\'2\\', fontname=\"times\"),\\n      engine=\\'dot\\')\\n  g.body.extend([\\'rankdir=LR\\'])\\n\\n  g.node(\"c_{k-2}\", fillcolor=\\'darkseagreen2\\')\\n  g.node(\"c_{k-1}\", fillcolor=\\'darkseagreen2\\')\\n  assert len(genotype) % 2 == 0\\n  steps = len(genotype) // 2\\n\\n  for i in range(steps):\\n    g.node(str(i), fillcolor=\\'lightblue\\')\\n\\n  for i in range(steps):\\n    for k in [2*i, 2*i + 1]:\\n      op, j = genotype[k]\\n      if j == 0:\\n        u = \"c_{k-2}\"\\n      elif j == 1:\\n        u = \"c_{k-1}\"\\n      else:\\n        u = str(j-2)\\n      v = str(i)\\n      g.edge(u, v, label=op, fillcolor=\"gray\")\\n\\n  g.node(\"c_{k}\", fillcolor=\\'palegoldenrod\\')\\n  for i in range(steps):\\n    g.edge(str(i), \"c_{k}\", fillcolor=\"gray\")\\n\\n  g.render(filename, view=True)\\n\\n\\nif __name__ == \\'__main__\\':\\n  if len(sys.argv) != 2:\\n    print(\"usage:\\\\n python {} ARCH_NAME\".format(sys.argv[0]))\\n    sys.exit(1)\\n\\n  genotype_name = sys.argv[1]\\n  try:\\n    genotype = eval(\\'genotypes.{}\\'.format(genotype_name))\\n  except AttributeError:\\n    print(\"{} is not specified in genotypes.py\".format(genotype_name)) \\n    sys.exit(1)\\n\\n  plot(genotype.normal, \"normal\")\\n  plot(genotype.reduce, \"reduction\")\\n\\nimport os\\nimport sys\\nimport glob\\nimport numpy as np\\nimport torch\\nimport utils\\nimport logging\\nimport argparse\\nimport torch.nn as nn\\nimport genotypes\\nimport torch.utils\\nimport torchvision.datasets as dset\\nimport torch.backends.cudnn as cudnn\\n\\nfrom torch.autograd import Variable\\nfrom model import NetworkCIFAR as Network\\n\\n\\nparser = argparse.ArgumentParser(\"cifar\")\\nparser.add_argument(\\'--data\\', type=str, default=\\'../data\\', help=\\'location of the data corpus\\')\\nparser.add_argument(\\'--batch_size\\', type=int, default=96, help=\\'batch size\\')\\nparser.add_argument(\\'--report_freq\\', type=float, default=50, help=\\'report frequency\\')\\nparser.add_argument(\\'--gpu\\', type=int, default=0, help=\\'gpu device id\\')\\nparser.add_argument(\\'--init_channels\\', type=int, default=36, help=\\'num of init channels\\')\\nparser.add_argument(\\'--layers\\', type=int, default=20, help=\\'total number of layers\\')\\nparser.add_argument(\\'--model_path\\', type=str, default=\\'EXP/model.pt\\', help=\\'path of pretrained model\\')\\nparser.add_argument(\\'--auxiliary\\', action=\\'store_true\\', default=False, help=\\'use auxiliary tower\\')\\nparser.add_argument(\\'--cutout\\', action=\\'store_true\\', default=False, help=\\'use cutout\\')\\nparser.add_argument(\\'--cutout_length\\', type=int, default=16, help=\\'cutout length\\')\\nparser.add_argument(\\'--drop_path_prob\\', type=float, default=0.2, help=\\'drop path probability\\')\\nparser.add_argument(\\'--seed\\', type=int, default=0, help=\\'random seed\\')\\nparser.add_argument(\\'--arch\\', type=str, default=\\'DARTS\\', help=\\'which architecture to use\\')\\nargs = parser.parse_args()\\n\\nlog_format = \\'%(asctime)s %(message)s\\'\\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO,\\n    format=log_format, datefmt=\\'%m/%d %I:%M:%S %p\\')\\n\\nCIFAR_CLASSES = 10\\n\\n\\ndef main():\\n  if not torch.cuda.is_available():\\n    logging.info(\\'no gpu device available\\')\\n    sys.exit(1)\\n\\n  np.random.seed(args.seed)\\n  torch.cuda.set_device(args.gpu)\\n  cudnn.benchmark = True\\n  torch.manual_seed(args.seed)\\n  cudnn.enabled=True\\n  torch.cuda.manual_seed(args.seed)\\n  logging.info(\\'gpu device = %d\\' % args.gpu)\\n  logging.info(\"args = %s\", args)\\n\\n  genotype = eval(\"genotypes.%s\" % args.arch)\\n  model = Network(args.init_channels, CIFAR_CLASSES, args.layers, args.auxiliary, genotype)\\n  model = model.cuda()\\n  utils.load(model, args.model_path)\\n\\n  logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\\n\\n  criterion = nn.CrossEntropyLoss()\\n  criterion = criterion.cuda()\\n\\n  _, test_transform = utils._data_transforms_cifar10(args)\\n  test_data = dset.CIFAR10(root=args.data, train=False, download=True, transform=test_transform)\\n\\n  test_queue = torch.utils.data.DataLoader(\\n      test_data, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=2)\\n\\n  model.drop_path_prob = args.drop_path_prob\\n  test_acc, test_obj = infer(test_queue, model, criterion)\\n  logging.info(\\'test_acc %f\\', test_acc)\\n\\n\\ndef infer(test_queue, model, criterion):\\n  objs = utils.AvgrageMeter()\\n  top1 = utils.AvgrageMeter()\\n  top5 = utils.AvgrageMeter()\\n  model.eval()\\n\\n  for step, (input, target) in enumerate(test_queue):\\n    input = Variable(input, volatile=True).cuda()\\n    target = Variable(target, volatile=True).cuda(async=True)\\n\\n    logits, _ = model(input)\\n    loss = criterion(logits, target)\\n\\n    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\\n    n = input.size(0)\\n    objs.update(loss.data[0], n)\\n    top1.update(prec1.data[0], n)\\n    top5.update(prec5.data[0], n)\\n\\n    if step % args.report_freq == 0:\\n      logging.info(\\'test %03d %e %f %f\\', step, objs.avg, top1.avg, top5.avg)\\n\\n  return top1.avg, objs.avg\\n\\n\\nif __name__ == \\'__main__\\':\\n  main() \\n\\nimport torch\\nimport numpy as np\\nimport torch.nn as nn\\nfrom torch.autograd import Variable\\n\\n\\ndef _concat(xs):\\n  return torch.cat([x.view(-1) for x in xs])\\n\\n\\nclass Architect(object):\\n\\n  def __init__(self, model, args):\\n    self.network_momentum = args.momentum\\n    self.network_weight_decay = args.weight_decay\\n    self.model = model\\n    self.optimizer = torch.optim.Adam(self.model.arch_parameters(),\\n        lr=args.arch_learning_rate, betas=(0.5, 0.999), weight_decay=args.arch_weight_decay)\\n\\n  def _compute_unrolled_model(self, input, target, eta, network_optimizer):\\n    loss = self.model._loss(input, target)\\n    theta = _concat(self.model.parameters()).data\\n    try:\\n      moment = _concat(network_optimizer.state[v][\\'momentum_buffer\\'] for v in self.model.parameters()).mul_(self.network_momentum)\\n    except:\\n      moment = torch.zeros_like(theta)\\n    dtheta = _concat(torch.autograd.grad(loss, self.model.parameters())).data + self.network_weight_decay*theta\\n    unrolled_model = self._construct_model_from_theta(theta.sub(eta, moment+dtheta))\\n    return unrolled_model\\n\\n  def step(self, input_train, target_train, input_valid, target_valid, eta, network_optimizer, unrolled):\\n    self.optimizer.zero_grad()\\n    if unrolled:\\n        self._backward_step_unrolled(input_train, target_train, input_valid, target_valid, eta, network_optimizer)\\n    else:\\n        self._backward_step(input_valid, target_valid)\\n    self.optimizer.step()\\n\\n  def _backward_step(self, input_valid, target_valid):\\n    loss = self.model._loss(input_valid, target_valid)\\n    loss.backward()\\n\\n  def _backward_step_unrolled(self, input_train, target_train, input_valid, target_valid, eta, network_optimizer):\\n    unrolled_model = self._compute_unrolled_model(input_train, target_train, eta, network_optimizer)\\n    unrolled_loss = unrolled_model._loss(input_valid, target_valid)\\n\\n    unrolled_loss.backward()\\n    dalpha = [v.grad for v in unrolled_model.arch_parameters()]\\n    vector = [v.grad.data for v in unrolled_model.parameters()]\\n    implicit_grads = self._hessian_vector_product(vector, input_train, target_train)\\n\\n    for g, ig in zip(dalpha, implicit_grads):\\n      g.data.sub_(eta, ig.data)\\n\\n    for v, g in zip(self.model.arch_parameters(), dalpha):\\n      if v.grad is None:\\n        v.grad = Variable(g.data)\\n      else:\\n        v.grad.data.copy_(g.data)\\n\\n  def _construct_model_from_theta(self, theta):\\n    model_new = self.model.new()\\n    model_dict = self.model.state_dict()\\n\\n    params, offset = {}, 0\\n    for k, v in self.model.named_parameters():\\n      v_length = np.prod(v.size())\\n      params[k] = theta[offset: offset+v_length].view(v.size())\\n      offset += v_length\\n\\n    assert offset == len(theta)\\n    model_dict.update(params)\\n    model_new.load_state_dict(model_dict)\\n    return model_new.cuda()\\n\\n  def _hessian_vector_product(self, vector, input, target, r=1e-2):\\n    R = r / _concat(vector).norm()\\n    for p, v in zip(self.model.parameters(), vector):\\n      p.data.add_(R, v)\\n    loss = self.model._loss(input, target)\\n    grads_p = torch.autograd.grad(loss, self.model.arch_parameters())\\n\\n    for p, v in zip(self.model.parameters(), vector):\\n      p.data.sub_(2*R, v)\\n    loss = self.model._loss(input, target)\\n    grads_n = torch.autograd.grad(loss, self.model.arch_parameters())\\n\\n    for p, v in zip(self.model.parameters(), vector):\\n      p.data.add_(R, v)\\n\\n    return [(x-y).div_(2*R) for x, y in zip(grads_p, grads_n)]\\n\\nimport torch\\nimport torch.nn as nn\\nfrom operations import *\\nfrom torch.autograd import Variable\\nfrom utils import drop_path\\n\\n\\nclass Cell(nn.Module):\\n\\n  def __init__(self, genotype, C_prev_prev, C_prev, C, reduction, reduction_prev):\\n    super(Cell, self).__init__()\\n    print(C_prev_prev, C_prev, C)\\n\\n    if reduction_prev:\\n      self.preprocess0 = FactorizedReduce(C_prev_prev, C)\\n    else:\\n      self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0)\\n    self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0)\\n    \\n    if reduction:\\n      op_names, indices = zip(*genotype.reduce)\\n      concat = genotype.reduce_concat\\n    else:\\n      op_names, indices = zip(*genotype.normal)\\n      concat = genotype.normal_concat\\n    self._compile(C, op_names, indices, concat, reduction)\\n\\n  def _compile(self, C, op_names, indices, concat, reduction):\\n    assert len(op_names) == len(indices)\\n    self._steps = len(op_names) // 2\\n    self._concat = concat\\n    self.multiplier = len(concat)\\n\\n    self._ops = nn.ModuleList()\\n    for name, index in zip(op_names, indices):\\n      stride = 2 if reduction and index < 2 else 1\\n      op = OPS[name](C, stride, True)\\n      self._ops += [op]\\n    self._indices = indices\\n\\n  def forward(self, s0, s1, drop_prob):\\n    s0 = self.preprocess0(s0)\\n    s1 = self.preprocess1(s1)\\n\\n    states = [s0, s1]\\n    for i in range(self._steps):\\n      h1 = states[self._indices[2*i]]\\n      h2 = states[self._indices[2*i+1]]\\n      op1 = self._ops[2*i]\\n      op2 = self._ops[2*i+1]\\n      h1 = op1(h1)\\n      h2 = op2(h2)\\n      if self.training and drop_prob > 0.:\\n        if not isinstance(op1, Identity):\\n          h1 = drop_path(h1, drop_prob)\\n        if not isinstance(op2, Identity):\\n          h2 = drop_path(h2, drop_prob)\\n      s = h1 + h2\\n      states += [s]\\n    return torch.cat([states[i] for i in self._concat], dim=1)\\n\\n\\nclass AuxiliaryHeadCIFAR(nn.Module):\\n\\n  def __init__(self, C, num_classes):\\n    \"\"\"assuming input size 8x8\"\"\"\\n    super(AuxiliaryHeadCIFAR, self).__init__()\\n    self.features = nn.Sequential(\\n      nn.ReLU(inplace=True),\\n      nn.AvgPool2d(5, stride=3, padding=0, count_include_pad=False), # image size = 2 x 2\\n      nn.Conv2d(C, 128, 1, bias=False),\\n      nn.BatchNorm2d(128),\\n      nn.ReLU(inplace=True),\\n      nn.Conv2d(128, 768, 2, bias=False),\\n      nn.BatchNorm2d(768),\\n      nn.ReLU(inplace=True)\\n    )\\n    self.classifier = nn.Linear(768, num_classes)\\n\\n  def forward(self, x):\\n    x = self.features(x)\\n    x = self.classifier(x.view(x.size(0),-1))\\n    return x\\n\\n\\nclass AuxiliaryHeadImageNet(nn.Module):\\n\\n  def __init__(self, C, num_classes):\\n    \"\"\"assuming input size 14x14\"\"\"\\n    super(AuxiliaryHeadImageNet, self).__init__()\\n    self.features = nn.Sequential(\\n      nn.ReLU(inplace=True),\\n      nn.AvgPool2d(5, stride=2, padding=0, count_include_pad=False),\\n      nn.Conv2d(C, 128, 1, bias=False),\\n      nn.BatchNorm2d(128),\\n      nn.ReLU(inplace=True),\\n      nn.Conv2d(128, 768, 2, bias=False),\\n      # NOTE: This batchnorm was omitted in my earlier implementation due to a typo.\\n      # Commenting it out for consistency with the experiments in the paper.\\n      # nn.BatchNorm2d(768),\\n      nn.ReLU(inplace=True)\\n    )\\n    self.classifier = nn.Linear(768, num_classes)\\n\\n  def forward(self, x):\\n    x = self.features(x)\\n    x = self.classifier(x.view(x.size(0),-1))\\n    return x\\n\\n\\nclass NetworkCIFAR(nn.Module):\\n\\n  def __init__(self, C, num_classes, layers, auxiliary, genotype):\\n    super(NetworkCIFAR, self).__init__()\\n    self._layers = layers\\n    self._auxiliary = auxiliary\\n\\n    stem_multiplier = 3\\n    C_curr = stem_multiplier*C\\n    self.stem = nn.Sequential(\\n      nn.Conv2d(3, C_curr, 3, padding=1, bias=False),\\n      nn.BatchNorm2d(C_curr)\\n    )\\n    \\n    C_prev_prev, C_prev, C_curr = C_curr, C_curr, C\\n    self.cells = nn.ModuleList()\\n    reduction_prev = False\\n    for i in range(layers):\\n      if i in [layers//3, 2*layers//3]:\\n        C_curr *= 2\\n        reduction = True\\n      else:\\n        reduction = False\\n      cell = Cell(genotype, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)\\n      reduction_prev = reduction\\n      self.cells += [cell]\\n      C_prev_prev, C_prev = C_prev, cell.multiplier*C_curr\\n      if i == 2*layers//3:\\n        C_to_auxiliary = C_prev\\n\\n    if auxiliary:\\n      self.auxiliary_head = AuxiliaryHeadCIFAR(C_to_auxiliary, num_classes)\\n    self.global_pooling = nn.AdaptiveAvgPool2d(1)\\n    self.classifier = nn.Linear(C_prev, num_classes)\\n\\n  def forward(self, input):\\n    logits_aux = None\\n    s0 = s1 = self.stem(input)\\n    for i, cell in enumerate(self.cells):\\n      s0, s1 = s1, cell(s0, s1, self.drop_path_prob)\\n      if i == 2*self._layers//3:\\n        if self._auxiliary and self.training:\\n          logits_aux = self.auxiliary_head(s1)\\n    out = self.global_pooling(s1)\\n    logits = self.classifier(out.view(out.size(0),-1))\\n    return logits, logits_aux\\n\\n\\nclass NetworkImageNet(nn.Module):\\n\\n  def __init__(self, C, num_classes, layers, auxiliary, genotype):\\n    super(NetworkImageNet, self).__init__()\\n    self._layers = layers\\n    self._auxiliary = auxiliary\\n\\n    self.stem0 = nn.Sequential(\\n      nn.Conv2d(3, C // 2, kernel_size=3, stride=2, padding=1, bias=False),\\n      nn.BatchNorm2d(C // 2),\\n      nn.ReLU(inplace=True),\\n      nn.Conv2d(C // 2, C, 3, stride=2, padding=1, bias=False),\\n      nn.BatchNorm2d(C),\\n    )\\n\\n    self.stem1 = nn.Sequential(\\n      nn.ReLU(inplace=True),\\n      nn.Conv2d(C, C, 3, stride=2, padding=1, bias=False),\\n      nn.BatchNorm2d(C),\\n    )\\n\\n    C_prev_prev, C_prev, C_curr = C, C, C\\n\\n    self.cells = nn.ModuleList()\\n    reduction_prev = True\\n    for i in range(layers):\\n      if i in [layers // 3, 2 * layers // 3]:\\n        C_curr *= 2\\n        reduction = True\\n      else:\\n        reduction = False\\n      cell = Cell(genotype, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)\\n      reduction_prev = reduction\\n      self.cells += [cell]\\n      C_prev_prev, C_prev = C_prev, cell.multiplier * C_curr\\n      if i == 2 * layers // 3:\\n        C_to_auxiliary = C_prev\\n\\n    if auxiliary:\\n      self.auxiliary_head = AuxiliaryHeadImageNet(C_to_auxiliary, num_classes)\\n    self.global_pooling = nn.AvgPool2d(7)\\n    self.classifier = nn.Linear(C_prev, num_classes)\\n\\n  def forward(self, input):\\n    logits_aux = None\\n    s0 = self.stem0(input)\\n    s1 = self.stem1(s0)\\n    for i, cell in enumerate(self.cells):\\n      s0, s1 = s1, cell(s0, s1, self.drop_path_prob)\\n      if i == 2 * self._layers // 3:\\n        if self._auxiliary and self.training:\\n          logits_aux = self.auxiliary_head(s1)\\n    out = self.global_pooling(s1)\\n    logits = self.classifier(out.view(out.size(0), -1))\\n    return logits, logits_aux\\n\\nimport torch\\nimport torch.nn as nn\\n\\nOPS = {\\n  \\'none\\' : lambda C, stride, affine: Zero(stride),\\n  \\'avg_pool_3x3\\' : lambda C, stride, affine: nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False),\\n  \\'max_pool_3x3\\' : lambda C, stride, affine: nn.MaxPool2d(3, stride=stride, padding=1),\\n  \\'skip_connect\\' : lambda C, stride, affine: Identity() if stride == 1 else FactorizedReduce(C, C, affine=affine),\\n  \\'sep_conv_3x3\\' : lambda C, stride, affine: SepConv(C, C, 3, stride, 1, affine=affine),\\n  \\'sep_conv_5x5\\' : lambda C, stride, affine: SepConv(C, C, 5, stride, 2, affine=affine),\\n  \\'sep_conv_7x7\\' : lambda C, stride, affine: SepConv(C, C, 7, stride, 3, affine=affine),\\n  \\'dil_conv_3x3\\' : lambda C, stride, affine: DilConv(C, C, 3, stride, 2, 2, affine=affine),\\n  \\'dil_conv_5x5\\' : lambda C, stride, affine: DilConv(C, C, 5, stride, 4, 2, affine=affine),\\n  \\'conv_7x1_1x7\\' : lambda C, stride, affine: nn.Sequential(\\n    nn.ReLU(inplace=False),\\n    nn.Conv2d(C, C, (1,7), stride=(1, stride), padding=(0, 3), bias=False),\\n    nn.Conv2d(C, C, (7,1), stride=(stride, 1), padding=(3, 0), bias=False),\\n    nn.BatchNorm2d(C, affine=affine)\\n    ),\\n}\\n\\nclass ReLUConvBN(nn.Module):\\n\\n  def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\\n    super(ReLUConvBN, self).__init__()\\n    self.op = nn.Sequential(\\n      nn.ReLU(inplace=False),\\n      nn.Conv2d(C_in, C_out, kernel_size, stride=stride, padding=padding, bias=False),\\n      nn.BatchNorm2d(C_out, affine=affine)\\n    )\\n\\n  def forward(self, x):\\n    return self.op(x)\\n\\nclass DilConv(nn.Module):\\n    \\n  def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):\\n    super(DilConv, self).__init__()\\n    self.op = nn.Sequential(\\n      nn.ReLU(inplace=False),\\n      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=C_in, bias=False),\\n      nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),\\n      nn.BatchNorm2d(C_out, affine=affine),\\n      )\\n\\n  def forward(self, x):\\n    return self.op(x)\\n\\n\\nclass SepConv(nn.Module):\\n    \\n  def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\\n    super(SepConv, self).__init__()\\n    self.op = nn.Sequential(\\n      nn.ReLU(inplace=False),\\n      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, groups=C_in, bias=False),\\n      nn.Conv2d(C_in, C_in, kernel_size=1, padding=0, bias=False),\\n      nn.BatchNorm2d(C_in, affine=affine),\\n      nn.ReLU(inplace=False),\\n      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=1, padding=padding, groups=C_in, bias=False),\\n      nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),\\n      nn.BatchNorm2d(C_out, affine=affine),\\n      )\\n\\n  def forward(self, x):\\n    return self.op(x)\\n\\n\\nclass Identity(nn.Module):\\n\\n  def __init__(self):\\n    super(Identity, self).__init__()\\n\\n  def forward(self, x):\\n    return x\\n\\n\\nclass Zero(nn.Module):\\n\\n  def __init__(self, stride):\\n    super(Zero, self).__init__()\\n    self.stride = stride\\n\\n  def forward(self, x):\\n    if self.stride == 1:\\n      return x.mul(0.)\\n    return x[:,:,::self.stride,::self.stride].mul(0.)\\n\\n\\nclass FactorizedReduce(nn.Module):\\n\\n  def __init__(self, C_in, C_out, affine=True):\\n    super(FactorizedReduce, self).__init__()\\n    assert C_out % 2 == 0\\n    self.relu = nn.ReLU(inplace=False)\\n    self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\\n    self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False) \\n    self.bn = nn.BatchNorm2d(C_out, affine=affine)\\n\\n  def forward(self, x):\\n    x = self.relu(x)\\n    out = torch.cat([self.conv_1(x), self.conv_2(x[:,:,1:,1:])], dim=1)\\n    out = self.bn(out)\\n    return out\\n\\nimport os\\nimport numpy as np\\nimport torch\\nimport shutil\\nimport torchvision.transforms as transforms\\nfrom torch.autograd import Variable\\n\\n\\nclass AvgrageMeter(object):\\n\\n  def __init__(self):\\n    self.reset()\\n\\n  def reset(self):\\n    self.avg = 0\\n    self.sum = 0\\n    self.cnt = 0\\n\\n  def update(self, val, n=1):\\n    self.sum += val * n\\n    self.cnt += n\\n    self.avg = self.sum / self.cnt\\n\\n\\ndef accuracy(output, target, topk=(1,)):\\n  maxk = max(topk)\\n  batch_size = target.size(0)\\n\\n  _, pred = output.topk(maxk, 1, True, True)\\n  pred = pred.t()\\n  correct = pred.eq(target.view(1, -1).expand_as(pred))\\n\\n  res = []\\n  for k in topk:\\n    correct_k = correct[:k].view(-1).float().sum(0)\\n    res.append(correct_k.mul_(100.0/batch_size))\\n  return res\\n\\n\\nclass Cutout(object):\\n    def __init__(self, length):\\n        self.length = length\\n\\n    def __call__(self, img):\\n        h, w = img.size(1), img.size(2)\\n        mask = np.ones((h, w), np.float32)\\n        y = np.random.randint(h)\\n        x = np.random.randint(w)\\n\\n        y1 = np.clip(y - self.length // 2, 0, h)\\n        y2 = np.clip(y + self.length // 2, 0, h)\\n        x1 = np.clip(x - self.length // 2, 0, w)\\n        x2 = np.clip(x + self.length // 2, 0, w)\\n\\n        mask[y1: y2, x1: x2] = 0.\\n        mask = torch.from_numpy(mask)\\n        mask = mask.expand_as(img)\\n        img *= mask\\n        return img\\n\\n\\ndef _data_transforms_cifar10(args):\\n  CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]\\n  CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\\n\\n  train_transform = transforms.Compose([\\n    transforms.RandomCrop(32, padding=4),\\n    transforms.RandomHorizontalFlip(),\\n    transforms.ToTensor(),\\n    transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\\n  ])\\n  if args.cutout:\\n    train_transform.transforms.append(Cutout(args.cutout_length))\\n\\n  valid_transform = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\\n    ])\\n  return train_transform, valid_transform\\n\\n\\ndef count_parameters_in_MB(model):\\n  return np.sum(np.prod(v.size()) for name, v in model.named_parameters() if \"auxiliary\" not in name)/1e6\\n\\n\\ndef save_checkpoint(state, is_best, save):\\n  filename = os.path.join(save, \\'checkpoint.pth.tar\\')\\n  torch.save(state, filename)\\n  if is_best:\\n    best_filename = os.path.join(save, \\'model_best.pth.tar\\')\\n    shutil.copyfile(filename, best_filename)\\n\\n\\ndef save(model, model_path):\\n  torch.save(model.state_dict(), model_path)\\n\\n\\ndef load(model, model_path):\\n  model.load_state_dict(torch.load(model_path))\\n\\n\\ndef drop_path(x, drop_prob):\\n  if drop_prob > 0.:\\n    keep_prob = 1.-drop_prob\\n    mask = Variable(torch.cuda.FloatTensor(x.size(0), 1, 1, 1).bernoulli_(keep_prob))\\n    x.div_(keep_prob)\\n    x.mul_(mask)\\n  return x\\n\\n\\ndef create_exp_dir(path, scripts_to_save=None):\\n  if not os.path.exists(path):\\n    os.mkdir(path)\\n  print(\\'Experiment dir : {}\\'.format(path))\\n\\n  if scripts_to_save is not None:\\n    os.mkdir(os.path.join(path, \\'scripts\\'))\\n    for script in scripts_to_save:\\n      dst_file = os.path.join(path, \\'scripts\\', os.path.basename(script))\\n      shutil.copyfile(script, dst_file)\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom operations import *\\nfrom torch.autograd import Variable\\nfrom genotypes import PRIMITIVES\\nfrom genotypes import Genotype\\n\\n\\nclass MixedOp(nn.Module):\\n\\n  def __init__(self, C, stride):\\n    super(MixedOp, self).__init__()\\n    self._ops = nn.ModuleList()\\n    for primitive in PRIMITIVES:\\n      op = OPS[primitive](C, stride, False)\\n      if \\'pool\\' in primitive:\\n        op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))\\n      self._ops.append(op)\\n\\n  def forward(self, x, weights):\\n    return sum(w * op(x) for w, op in zip(weights, self._ops))\\n\\n\\nclass Cell(nn.Module):\\n\\n  def __init__(self, steps, multiplier, C_prev_prev, C_prev, C, reduction, reduction_prev):\\n    super(Cell, self).__init__()\\n    self.reduction = reduction\\n\\n    if reduction_prev:\\n      self.preprocess0 = FactorizedReduce(C_prev_prev, C, affine=False)\\n    else:\\n      self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0, affine=False)\\n    self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0, affine=False)\\n    self._steps = steps\\n    self._multiplier = multiplier\\n\\n    self._ops = nn.ModuleList()\\n    self._bns = nn.ModuleList()\\n    for i in range(self._steps):\\n      for j in range(2+i):\\n        stride = 2 if reduction and j < 2 else 1\\n        op = MixedOp(C, stride)\\n        self._ops.append(op)\\n\\n  def forward(self, s0, s1, weights):\\n    s0 = self.preprocess0(s0)\\n    s1 = self.preprocess1(s1)\\n\\n    states = [s0, s1]\\n    offset = 0\\n    for i in range(self._steps):\\n      s = sum(self._ops[offset+j](h, weights[offset+j]) for j, h in enumerate(states))\\n      offset += len(states)\\n      states.append(s)\\n\\n    return torch.cat(states[-self._multiplier:], dim=1)\\n\\n\\nclass Network(nn.Module):\\n\\n  def __init__(self, C, num_classes, layers, criterion, steps=4, multiplier=4, stem_multiplier=3):\\n    super(Network, self).__init__()\\n    self._C = C\\n    self._num_classes = num_classes\\n    self._layers = layers\\n    self._criterion = criterion\\n    self._steps = steps\\n    self._multiplier = multiplier\\n\\n    C_curr = stem_multiplier*C\\n    self.stem = nn.Sequential(\\n      nn.Conv2d(3, C_curr, 3, padding=1, bias=False),\\n      nn.BatchNorm2d(C_curr)\\n    )\\n \\n    C_prev_prev, C_prev, C_curr = C_curr, C_curr, C\\n    self.cells = nn.ModuleList()\\n    reduction_prev = False\\n    for i in range(layers):\\n      if i in [layers//3, 2*layers//3]:\\n        C_curr *= 2\\n        reduction = True\\n      else:\\n        reduction = False\\n      cell = Cell(steps, multiplier, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)\\n      reduction_prev = reduction\\n      self.cells += [cell]\\n      C_prev_prev, C_prev = C_prev, multiplier*C_curr\\n\\n    self.global_pooling = nn.AdaptiveAvgPool2d(1)\\n    self.classifier = nn.Linear(C_prev, num_classes)\\n\\n    self._initialize_alphas()\\n\\n  def new(self):\\n    model_new = Network(self._C, self._num_classes, self._layers, self._criterion).cuda()\\n    for x, y in zip(model_new.arch_parameters(), self.arch_parameters()):\\n        x.data.copy_(y.data)\\n    return model_new\\n\\n  def forward(self, input):\\n    s0 = s1 = self.stem(input)\\n    for i, cell in enumerate(self.cells):\\n      if cell.reduction:\\n        weights = F.softmax(self.alphas_reduce, dim=-1)\\n      else:\\n        weights = F.softmax(self.alphas_normal, dim=-1)\\n      s0, s1 = s1, cell(s0, s1, weights)\\n    out = self.global_pooling(s1)\\n    logits = self.classifier(out.view(out.size(0),-1))\\n    return logits\\n\\n  def _loss(self, input, target):\\n    logits = self(input)\\n    return self._criterion(logits, target) \\n\\n  def _initialize_alphas(self):\\n    k = sum(1 for i in range(self._steps) for n in range(2+i))\\n    num_ops = len(PRIMITIVES)\\n\\n    self.alphas_normal = Variable(1e-3*torch.randn(k, num_ops).cuda(), requires_grad=True)\\n    self.alphas_reduce = Variable(1e-3*torch.randn(k, num_ops).cuda(), requires_grad=True)\\n    self._arch_parameters = [\\n      self.alphas_normal,\\n      self.alphas_reduce,\\n    ]\\n\\n  def arch_parameters(self):\\n    return self._arch_parameters\\n\\n  def genotype(self):\\n\\n    def _parse(weights):\\n      gene = []\\n      n = 2\\n      start = 0\\n      for i in range(self._steps):\\n        end = start + n\\n        W = weights[start:end].copy()\\n        edges = sorted(range(i + 2), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index(\\'none\\')))[:2]\\n        for j in edges:\\n          k_best = None\\n          for k in range(len(W[j])):\\n            if k != PRIMITIVES.index(\\'none\\'):\\n              if k_best is None or W[j][k] > W[j][k_best]:\\n                k_best = k\\n          gene.append((PRIMITIVES[k_best], j))\\n        start = end\\n        n += 1\\n      return gene\\n\\n    gene_normal = _parse(F.softmax(self.alphas_normal, dim=-1).data.cpu().numpy())\\n    gene_reduce = _parse(F.softmax(self.alphas_reduce, dim=-1).data.cpu().numpy())\\n\\n    concat = range(2+self._steps-self._multiplier, self._steps+2)\\n    genotype = Genotype(\\n      normal=gene_normal, normal_concat=concat,\\n      reduce=gene_reduce, reduce_concat=concat\\n    )\\n    return genotype\\n\\nimport os\\nimport sys\\nimport time\\nimport glob\\nimport numpy as np\\nimport torch\\nimport utils\\nimport logging\\nimport argparse\\nimport torch.nn as nn\\nimport genotypes\\nimport torch.utils\\nimport torchvision.datasets as dset\\nimport torch.backends.cudnn as cudnn\\n\\nfrom torch.autograd import Variable\\nfrom model import NetworkCIFAR as Network\\n\\n\\nparser = argparse.ArgumentParser(\"cifar\")\\nparser.add_argument(\\'--data\\', type=str, default=\\'../data\\', help=\\'location of the data corpus\\')\\nparser.add_argument(\\'--batch_size\\', type=int, default=96, help=\\'batch size\\')\\nparser.add_argument(\\'--learning_rate\\', type=float, default=0.025, help=\\'init learning rate\\')\\nparser.add_argument(\\'--momentum\\', type=float, default=0.9, help=\\'momentum\\')\\nparser.add_argument(\\'--weight_decay\\', type=float, default=3e-4, help=\\'weight decay\\')\\nparser.add_argument(\\'--report_freq\\', type=float, default=50, help=\\'report frequency\\')\\nparser.add_argument(\\'--gpu\\', type=int, default=0, help=\\'gpu device id\\')\\nparser.add_argument(\\'--epochs\\', type=int, default=600, help=\\'num of training epochs\\')\\nparser.add_argument(\\'--init_channels\\', type=int, default=36, help=\\'num of init channels\\')\\nparser.add_argument(\\'--layers\\', type=int, default=20, help=\\'total number of layers\\')\\nparser.add_argument(\\'--model_path\\', type=str, default=\\'saved_models\\', help=\\'path to save the model\\')\\nparser.add_argument(\\'--auxiliary\\', action=\\'store_true\\', default=False, help=\\'use auxiliary tower\\')\\nparser.add_argument(\\'--auxiliary_weight\\', type=float, default=0.4, help=\\'weight for auxiliary loss\\')\\nparser.add_argument(\\'--cutout\\', action=\\'store_true\\', default=False, help=\\'use cutout\\')\\nparser.add_argument(\\'--cutout_length\\', type=int, default=16, help=\\'cutout length\\')\\nparser.add_argument(\\'--drop_path_prob\\', type=float, default=0.2, help=\\'drop path probability\\')\\nparser.add_argument(\\'--save\\', type=str, default=\\'EXP\\', help=\\'experiment name\\')\\nparser.add_argument(\\'--seed\\', type=int, default=0, help=\\'random seed\\')\\nparser.add_argument(\\'--arch\\', type=str, default=\\'DARTS\\', help=\\'which architecture to use\\')\\nparser.add_argument(\\'--grad_clip\\', type=float, default=5, help=\\'gradient clipping\\')\\nargs = parser.parse_args()\\n\\nargs.save = \\'eval-{}-{}\\'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\\nutils.create_exp_dir(args.save, scripts_to_save=glob.glob(\\'*.py\\'))\\n\\nlog_format = \\'%(asctime)s %(message)s\\'\\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO,\\n    format=log_format, datefmt=\\'%m/%d %I:%M:%S %p\\')\\nfh = logging.FileHandler(os.path.join(args.save, \\'log.txt\\'))\\nfh.setFormatter(logging.Formatter(log_format))\\nlogging.getLogger().addHandler(fh)\\n\\nCIFAR_CLASSES = 10\\n\\n\\ndef main():\\n  if not torch.cuda.is_available():\\n    logging.info(\\'no gpu device available\\')\\n    sys.exit(1)\\n\\n  np.random.seed(args.seed)\\n  torch.cuda.set_device(args.gpu)\\n  cudnn.benchmark = True\\n  torch.manual_seed(args.seed)\\n  cudnn.enabled=True\\n  torch.cuda.manual_seed(args.seed)\\n  logging.info(\\'gpu device = %d\\' % args.gpu)\\n  logging.info(\"args = %s\", args)\\n\\n  genotype = eval(\"genotypes.%s\" % args.arch)\\n  model = Network(args.init_channels, CIFAR_CLASSES, args.layers, args.auxiliary, genotype)\\n  model = model.cuda()\\n\\n  logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\\n\\n  criterion = nn.CrossEntropyLoss()\\n  criterion = criterion.cuda()\\n  optimizer = torch.optim.SGD(\\n      model.parameters(),\\n      args.learning_rate,\\n      momentum=args.momentum,\\n      weight_decay=args.weight_decay\\n      )\\n\\n  train_transform, valid_transform = utils._data_transforms_cifar10(args)\\n  train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)\\n  valid_data = dset.CIFAR10(root=args.data, train=False, download=True, transform=valid_transform)\\n\\n  train_queue = torch.utils.data.DataLoader(\\n      train_data, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=2)\\n\\n  valid_queue = torch.utils.data.DataLoader(\\n      valid_data, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=2)\\n\\n  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, float(args.epochs))\\n\\n  for epoch in range(args.epochs):\\n    scheduler.step()\\n    logging.info(\\'epoch %d lr %e\\', epoch, scheduler.get_lr()[0])\\n    model.drop_path_prob = args.drop_path_prob * epoch / args.epochs\\n\\n    train_acc, train_obj = train(train_queue, model, criterion, optimizer)\\n    logging.info(\\'train_acc %f\\', train_acc)\\n\\n    valid_acc, valid_obj = infer(valid_queue, model, criterion)\\n    logging.info(\\'valid_acc %f\\', valid_acc)\\n\\n    utils.save(model, os.path.join(args.save, \\'weights.pt\\'))\\n\\n\\ndef train(train_queue, model, criterion, optimizer):\\n  objs = utils.AvgrageMeter()\\n  top1 = utils.AvgrageMeter()\\n  top5 = utils.AvgrageMeter()\\n  model.train()\\n\\n  for step, (input, target) in enumerate(train_queue):\\n    input = Variable(input).cuda()\\n    target = Variable(target).cuda(async=True)\\n\\n    optimizer.zero_grad()\\n    logits, logits_aux = model(input)\\n    loss = criterion(logits, target)\\n    if args.auxiliary:\\n      loss_aux = criterion(logits_aux, target)\\n      loss += args.auxiliary_weight*loss_aux\\n    loss.backward()\\n    nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\\n    optimizer.step()\\n\\n    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\\n    n = input.size(0)\\n    objs.update(loss.data[0], n)\\n    top1.update(prec1.data[0], n)\\n    top5.update(prec5.data[0], n)\\n\\n    if step % args.report_freq == 0:\\n      logging.info(\\'train %03d %e %f %f\\', step, objs.avg, top1.avg, top5.avg)\\n\\n  return top1.avg, objs.avg\\n\\n\\ndef infer(valid_queue, model, criterion):\\n  objs = utils.AvgrageMeter()\\n  top1 = utils.AvgrageMeter()\\n  top5 = utils.AvgrageMeter()\\n  model.eval()\\n\\n  for step, (input, target) in enumerate(valid_queue):\\n    input = Variable(input, volatile=True).cuda()\\n    target = Variable(target, volatile=True).cuda(async=True)\\n\\n    logits, _ = model(input)\\n    loss = criterion(logits, target)\\n\\n    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\\n    n = input.size(0)\\n    objs.update(loss.data[0], n)\\n    top1.update(prec1.data[0], n)\\n    top5.update(prec5.data[0], n)\\n\\n    if step % args.report_freq == 0:\\n      logging.info(\\'valid %03d %e %f %f\\', step, objs.avg, top1.avg, top5.avg)\\n\\n  return top1.avg, objs.avg\\n\\n\\nif __name__ == \\'__main__\\':\\n  main() \\n\\nfrom collections import namedtuple\\n\\nGenotype = namedtuple(\\'Genotype\\', \\'normal normal_concat reduce reduce_concat\\')\\n\\nPRIMITIVES = [\\n    \\'none\\',\\n    \\'max_pool_3x3\\',\\n    \\'avg_pool_3x3\\',\\n    \\'skip_connect\\',\\n    \\'sep_conv_3x3\\',\\n    \\'sep_conv_5x5\\',\\n    \\'dil_conv_3x3\\',\\n    \\'dil_conv_5x5\\'\\n]\\n\\nNASNet = Genotype(\\n  normal = [\\n    (\\'sep_conv_5x5\\', 1),\\n    (\\'sep_conv_3x3\\', 0),\\n    (\\'sep_conv_5x5\\', 0),\\n    (\\'sep_conv_3x3\\', 0),\\n    (\\'avg_pool_3x3\\', 1),\\n    (\\'skip_connect\\', 0),\\n    (\\'avg_pool_3x3\\', 0),\\n    (\\'avg_pool_3x3\\', 0),\\n    (\\'sep_conv_3x3\\', 1),\\n    (\\'skip_connect\\', 1),\\n  ],\\n  normal_concat = [2, 3, 4, 5, 6],\\n  reduce = [\\n    (\\'sep_conv_5x5\\', 1),\\n    (\\'sep_conv_7x7\\', 0),\\n    (\\'max_pool_3x3\\', 1),\\n    (\\'sep_conv_7x7\\', 0),\\n    (\\'avg_pool_3x3\\', 1),\\n    (\\'sep_conv_5x5\\', 0),\\n    (\\'skip_connect\\', 3),\\n    (\\'avg_pool_3x3\\', 2),\\n    (\\'sep_conv_3x3\\', 2),\\n    (\\'max_pool_3x3\\', 1),\\n  ],\\n  reduce_concat = [4, 5, 6],\\n)\\n    \\nAmoebaNet = Genotype(\\n  normal = [\\n    (\\'avg_pool_3x3\\', 0),\\n    (\\'max_pool_3x3\\', 1),\\n    (\\'sep_conv_3x3\\', 0),\\n    (\\'sep_conv_5x5\\', 2),\\n    (\\'sep_conv_3x3\\', 0),\\n    (\\'avg_pool_3x3\\', 3),\\n    (\\'sep_conv_3x3\\', 1),\\n    (\\'skip_connect\\', 1),\\n    (\\'skip_connect\\', 0),\\n    (\\'avg_pool_3x3\\', 1),\\n    ],\\n  normal_concat = [4, 5, 6],\\n  reduce = [\\n    (\\'avg_pool_3x3\\', 0),\\n    (\\'sep_conv_3x3\\', 1),\\n    (\\'max_pool_3x3\\', 0),\\n    (\\'sep_conv_7x7\\', 2),\\n    (\\'sep_conv_7x7\\', 0),\\n    (\\'avg_pool_3x3\\', 1),\\n    (\\'max_pool_3x3\\', 0),\\n    (\\'max_pool_3x3\\', 1),\\n    (\\'conv_7x1_1x7\\', 0),\\n    (\\'sep_conv_3x3\\', 5),\\n  ],\\n  reduce_concat = [3, 4, 6]\\n)\\n\\nDARTS_V1 = Genotype(normal=[(\\'sep_conv_3x3\\', 1), (\\'sep_conv_3x3\\', 0), (\\'skip_connect\\', 0), (\\'sep_conv_3x3\\', 1), (\\'skip_connect\\', 0), (\\'sep_conv_3x3\\', 1), (\\'sep_conv_3x3\\', 0), (\\'skip_connect\\', 2)], normal_concat=[2, 3, 4, 5], reduce=[(\\'max_pool_3x3\\', 0), (\\'max_pool_3x3\\', 1), (\\'skip_connect\\', 2), (\\'max_pool_3x3\\', 0), (\\'max_pool_3x3\\', 0), (\\'skip_connect\\', 2), (\\'skip_connect\\', 2), (\\'avg_pool_3x3\\', 0)], reduce_concat=[2, 3, 4, 5])\\nDARTS_V2 = Genotype(normal=[(\\'sep_conv_3x3\\', 0), (\\'sep_conv_3x3\\', 1), (\\'sep_conv_3x3\\', 0), (\\'sep_conv_3x3\\', 1), (\\'sep_conv_3x3\\', 1), (\\'skip_connect\\', 0), (\\'skip_connect\\', 0), (\\'dil_conv_3x3\\', 2)], normal_concat=[2, 3, 4, 5], reduce=[(\\'max_pool_3x3\\', 0), (\\'max_pool_3x3\\', 1), (\\'skip_connect\\', 2), (\\'max_pool_3x3\\', 1), (\\'max_pool_3x3\\', 0), (\\'skip_connect\\', 2), (\\'skip_connect\\', 2), (\\'max_pool_3x3\\', 1)], reduce_concat=[2, 3, 4, 5])\\n\\nDARTS = DARTS_V2\\n\\nimport os\\nimport sys\\nimport time\\nimport glob\\nimport numpy as np\\nimport torch\\nimport utils\\nimport logging\\nimport argparse\\nimport torch.nn as nn\\nimport torch.utils\\nimport torch.nn.functional as F\\nimport torchvision.datasets as dset\\nimport torch.backends.cudnn as cudnn\\n\\nfrom torch.autograd import Variable\\nfrom model_search import Network\\nfrom architect import Architect\\n\\n\\nparser = argparse.ArgumentParser(\"cifar\")\\nparser.add_argument(\\'--data\\', type=str, default=\\'../data\\', help=\\'location of the data corpus\\')\\nparser.add_argument(\\'--batch_size\\', type=int, default=64, help=\\'batch size\\')\\nparser.add_argument(\\'--learning_rate\\', type=float, default=0.025, help=\\'init learning rate\\')\\nparser.add_argument(\\'--learning_rate_min\\', type=float, default=0.001, help=\\'min learning rate\\')\\nparser.add_argument(\\'--momentum\\', type=float, default=0.9, help=\\'momentum\\')\\nparser.add_argument(\\'--weight_decay\\', type=float, default=3e-4, help=\\'weight decay\\')\\nparser.add_argument(\\'--report_freq\\', type=float, default=50, help=\\'report frequency\\')\\nparser.add_argument(\\'--gpu\\', type=int, default=0, help=\\'gpu device id\\')\\nparser.add_argument(\\'--epochs\\', type=int, default=50, help=\\'num of training epochs\\')\\nparser.add_argument(\\'--init_channels\\', type=int, default=16, help=\\'num of init channels\\')\\nparser.add_argument(\\'--layers\\', type=int, default=8, help=\\'total number of layers\\')\\nparser.add_argument(\\'--model_path\\', type=str, default=\\'saved_models\\', help=\\'path to save the model\\')\\nparser.add_argument(\\'--cutout\\', action=\\'store_true\\', default=False, help=\\'use cutout\\')\\nparser.add_argument(\\'--cutout_length\\', type=int, default=16, help=\\'cutout length\\')\\nparser.add_argument(\\'--drop_path_prob\\', type=float, default=0.3, help=\\'drop path probability\\')\\nparser.add_argument(\\'--save\\', type=str, default=\\'EXP\\', help=\\'experiment name\\')\\nparser.add_argument(\\'--seed\\', type=int, default=2, help=\\'random seed\\')\\nparser.add_argument(\\'--grad_clip\\', type=float, default=5, help=\\'gradient clipping\\')\\nparser.add_argument(\\'--train_portion\\', type=float, default=0.5, help=\\'portion of training data\\')\\nparser.add_argument(\\'--unrolled\\', action=\\'store_true\\', default=False, help=\\'use one-step unrolled validation loss\\')\\nparser.add_argument(\\'--arch_learning_rate\\', type=float, default=3e-4, help=\\'learning rate for arch encoding\\')\\nparser.add_argument(\\'--arch_weight_decay\\', type=float, default=1e-3, help=\\'weight decay for arch encoding\\')\\nargs = parser.parse_args()\\n\\nargs.save = \\'search-{}-{}\\'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\\nutils.create_exp_dir(args.save, scripts_to_save=glob.glob(\\'*.py\\'))\\n\\nlog_format = \\'%(asctime)s %(message)s\\'\\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO,\\n    format=log_format, datefmt=\\'%m/%d %I:%M:%S %p\\')\\nfh = logging.FileHandler(os.path.join(args.save, \\'log.txt\\'))\\nfh.setFormatter(logging.Formatter(log_format))\\nlogging.getLogger().addHandler(fh)\\n\\n\\nCIFAR_CLASSES = 10\\n\\n\\ndef main():\\n  if not torch.cuda.is_available():\\n    logging.info(\\'no gpu device available\\')\\n    sys.exit(1)\\n\\n  np.random.seed(args.seed)\\n  torch.cuda.set_device(args.gpu)\\n  cudnn.benchmark = True\\n  torch.manual_seed(args.seed)\\n  cudnn.enabled=True\\n  torch.cuda.manual_seed(args.seed)\\n  logging.info(\\'gpu device = %d\\' % args.gpu)\\n  logging.info(\"args = %s\", args)\\n\\n  criterion = nn.CrossEntropyLoss()\\n  criterion = criterion.cuda()\\n  model = Network(args.init_channels, CIFAR_CLASSES, args.layers, criterion)\\n  model = model.cuda()\\n  logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\\n\\n  optimizer = torch.optim.SGD(\\n      model.parameters(),\\n      args.learning_rate,\\n      momentum=args.momentum,\\n      weight_decay=args.weight_decay)\\n\\n  train_transform, valid_transform = utils._data_transforms_cifar10(args)\\n  train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)\\n\\n  num_train = len(train_data)\\n  indices = list(range(num_train))\\n  split = int(np.floor(args.train_portion * num_train))\\n\\n  train_queue = torch.utils.data.DataLoader(\\n      train_data, batch_size=args.batch_size,\\n      sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\\n      pin_memory=True, num_workers=2)\\n\\n  valid_queue = torch.utils.data.DataLoader(\\n      train_data, batch_size=args.batch_size,\\n      sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\\n      pin_memory=True, num_workers=2)\\n\\n  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\\n        optimizer, float(args.epochs), eta_min=args.learning_rate_min)\\n\\n  architect = Architect(model, args)\\n\\n  for epoch in range(args.epochs):\\n    scheduler.step()\\n    lr = scheduler.get_lr()[0]\\n    logging.info(\\'epoch %d lr %e\\', epoch, lr)\\n\\n    genotype = model.genotype()\\n    logging.info(\\'genotype = %s\\', genotype)\\n\\n    print(F.softmax(model.alphas_normal, dim=-1))\\n    print(F.softmax(model.alphas_reduce, dim=-1))\\n\\n    # training\\n    train_acc, train_obj = train(train_queue, valid_queue, model, architect, criterion, optimizer, lr)\\n    logging.info(\\'train_acc %f\\', train_acc)\\n\\n    # validation\\n    valid_acc, valid_obj = infer(valid_queue, model, criterion)\\n    logging.info(\\'valid_acc %f\\', valid_acc)\\n\\n    utils.save(model, os.path.join(args.save, \\'weights.pt\\'))\\n\\n\\ndef train(train_queue, valid_queue, model, architect, criterion, optimizer, lr):\\n  objs = utils.AvgrageMeter()\\n  top1 = utils.AvgrageMeter()\\n  top5 = utils.AvgrageMeter()\\n\\n  for step, (input, target) in enumerate(train_queue):\\n    model.train()\\n    n = input.size(0)\\n\\n    input = Variable(input, requires_grad=False).cuda()\\n    target = Variable(target, requires_grad=False).cuda(async=True)\\n\\n    # get a random minibatch from the search queue with replacement\\n    input_search, target_search = next(iter(valid_queue))\\n    input_search = Variable(input_search, requires_grad=False).cuda()\\n    target_search = Variable(target_search, requires_grad=False).cuda(async=True)\\n\\n    architect.step(input, target, input_search, target_search, lr, optimizer, unrolled=args.unrolled)\\n\\n    optimizer.zero_grad()\\n    logits = model(input)\\n    loss = criterion(logits, target)\\n\\n    loss.backward()\\n    nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\\n    optimizer.step()\\n\\n    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\\n    objs.update(loss.data[0], n)\\n    top1.update(prec1.data[0], n)\\n    top5.update(prec5.data[0], n)\\n\\n    if step % args.report_freq == 0:\\n      logging.info(\\'train %03d %e %f %f\\', step, objs.avg, top1.avg, top5.avg)\\n\\n  return top1.avg, objs.avg\\n\\n\\ndef infer(valid_queue, model, criterion):\\n  objs = utils.AvgrageMeter()\\n  top1 = utils.AvgrageMeter()\\n  top5 = utils.AvgrageMeter()\\n  model.eval()\\n\\n  for step, (input, target) in enumerate(valid_queue):\\n    input = Variable(input, volatile=True).cuda()\\n    target = Variable(target, volatile=True).cuda(async=True)\\n\\n    logits = model(input)\\n    loss = criterion(logits, target)\\n\\n    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\\n    n = input.size(0)\\n    objs.update(loss.data[0], n)\\n    top1.update(prec1.data[0], n)\\n    top5.update(prec5.data[0], n)\\n\\n    if step % args.report_freq == 0:\\n      logging.info(\\'valid %03d %e %f %f\\', step, objs.avg, top1.avg, top5.avg)\\n\\n  return top1.avg, objs.avg\\n\\n\\nif __name__ == \\'__main__\\':\\n  main() \\n\\nimport os\\nimport sys\\nimport numpy as np\\nimport time\\nimport torch\\nimport utils\\nimport glob\\nimport random\\nimport logging\\nimport argparse\\nimport torch.nn as nn\\nimport genotypes\\nimport torch.utils\\nimport torchvision.datasets as dset\\nimport torchvision.transforms as transforms\\nimport torch.backends.cudnn as cudnn\\n\\nfrom torch.autograd import Variable\\nfrom model import NetworkImageNet as Network\\n\\n\\nparser = argparse.ArgumentParser(\"imagenet\")\\nparser.add_argument(\\'--data\\', type=str, default=\\'../data/imagenet/\\', help=\\'location of the data corpus\\')\\nparser.add_argument(\\'--batch_size\\', type=int, default=128, help=\\'batch size\\')\\nparser.add_argument(\\'--learning_rate\\', type=float, default=0.1, help=\\'init learning rate\\')\\nparser.add_argument(\\'--momentum\\', type=float, default=0.9, help=\\'momentum\\')\\nparser.add_argument(\\'--weight_decay\\', type=float, default=3e-5, help=\\'weight decay\\')\\nparser.add_argument(\\'--report_freq\\', type=float, default=100, help=\\'report frequency\\')\\nparser.add_argument(\\'--gpu\\', type=int, default=0, help=\\'gpu device id\\')\\nparser.add_argument(\\'--epochs\\', type=int, default=250, help=\\'num of training epochs\\')\\nparser.add_argument(\\'--init_channels\\', type=int, default=48, help=\\'num of init channels\\')\\nparser.add_argument(\\'--layers\\', type=int, default=14, help=\\'total number of layers\\')\\nparser.add_argument(\\'--auxiliary\\', action=\\'store_true\\', default=False, help=\\'use auxiliary tower\\')\\nparser.add_argument(\\'--auxiliary_weight\\', type=float, default=0.4, help=\\'weight for auxiliary loss\\')\\nparser.add_argument(\\'--drop_path_prob\\', type=float, default=0, help=\\'drop path probability\\')\\nparser.add_argument(\\'--save\\', type=str, default=\\'EXP\\', help=\\'experiment name\\')\\nparser.add_argument(\\'--seed\\', type=int, default=0, help=\\'random seed\\')\\nparser.add_argument(\\'--arch\\', type=str, default=\\'DARTS\\', help=\\'which architecture to use\\')\\nparser.add_argument(\\'--grad_clip\\', type=float, default=5., help=\\'gradient clipping\\')\\nparser.add_argument(\\'--label_smooth\\', type=float, default=0.1, help=\\'label smoothing\\')\\nparser.add_argument(\\'--gamma\\', type=float, default=0.97, help=\\'learning rate decay\\')\\nparser.add_argument(\\'--decay_period\\', type=int, default=1, help=\\'epochs between two learning rate decays\\')\\nparser.add_argument(\\'--parallel\\', action=\\'store_true\\', default=False, help=\\'data parallelism\\')\\nargs = parser.parse_args()\\n\\nargs.save = \\'eval-{}-{}\\'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\\nutils.create_exp_dir(args.save, scripts_to_save=glob.glob(\\'*.py\\'))\\n\\nlog_format = \\'%(asctime)s %(message)s\\'\\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO,\\n    format=log_format, datefmt=\\'%m/%d %I:%M:%S %p\\')\\nfh = logging.FileHandler(os.path.join(args.save, \\'log.txt\\'))\\nfh.setFormatter(logging.Formatter(log_format))\\nlogging.getLogger().addHandler(fh)\\n\\nCLASSES = 1000\\n\\n\\nclass CrossEntropyLabelSmooth(nn.Module):\\n\\n  def __init__(self, num_classes, epsilon):\\n    super(CrossEntropyLabelSmooth, self).__init__()\\n    self.num_classes = num_classes\\n    self.epsilon = epsilon\\n    self.logsoftmax = nn.LogSoftmax(dim=1)\\n\\n  def forward(self, inputs, targets):\\n    log_probs = self.logsoftmax(inputs)\\n    targets = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)\\n    targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\\n    loss = (-targets * log_probs).mean(0).sum()\\n    return loss\\n\\n\\ndef main():\\n  if not torch.cuda.is_available():\\n    logging.info(\\'no gpu device available\\')\\n    sys.exit(1)\\n\\n  np.random.seed(args.seed)\\n  torch.cuda.set_device(args.gpu)\\n  cudnn.benchmark = True\\n  torch.manual_seed(args.seed)\\n  cudnn.enabled=True\\n  torch.cuda.manual_seed(args.seed)\\n  logging.info(\\'gpu device = %d\\' % args.gpu)\\n  logging.info(\"args = %s\", args)\\n\\n  genotype = eval(\"genotypes.%s\" % args.arch)\\n  model = Network(args.init_channels, CLASSES, args.layers, args.auxiliary, genotype)\\n  if args.parallel:\\n    model = nn.DataParallel(model).cuda()\\n  else:\\n    model = model.cuda()\\n\\n  logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\\n\\n  criterion = nn.CrossEntropyLoss()\\n  criterion = criterion.cuda()\\n  criterion_smooth = CrossEntropyLabelSmooth(CLASSES, args.label_smooth)\\n  criterion_smooth = criterion_smooth.cuda()\\n\\n  optimizer = torch.optim.SGD(\\n    model.parameters(),\\n    args.learning_rate,\\n    momentum=args.momentum,\\n    weight_decay=args.weight_decay\\n    )\\n\\n  traindir = os.path.join(args.data, \\'train\\')\\n  validdir = os.path.join(args.data, \\'val\\')\\n  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n  train_data = dset.ImageFolder(\\n    traindir,\\n    transforms.Compose([\\n      transforms.RandomResizedCrop(224),\\n      transforms.RandomHorizontalFlip(),\\n      transforms.ColorJitter(\\n        brightness=0.4,\\n        contrast=0.4,\\n        saturation=0.4,\\n        hue=0.2),\\n      transforms.ToTensor(),\\n      normalize,\\n    ]))\\n  valid_data = dset.ImageFolder(\\n    validdir,\\n    transforms.Compose([\\n      transforms.Resize(256),\\n      transforms.CenterCrop(224),\\n      transforms.ToTensor(),\\n      normalize,\\n    ]))\\n\\n  train_queue = torch.utils.data.DataLoader(\\n    train_data, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=4)\\n\\n  valid_queue = torch.utils.data.DataLoader(\\n    valid_data, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=4)\\n\\n  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.decay_period, gamma=args.gamma)\\n\\n  best_acc_top1 = 0\\n  for epoch in range(args.epochs):\\n    scheduler.step()\\n    logging.info(\\'epoch %d lr %e\\', epoch, scheduler.get_lr()[0])\\n    model.drop_path_prob = args.drop_path_prob * epoch / args.epochs\\n\\n    train_acc, train_obj = train(train_queue, model, criterion_smooth, optimizer)\\n    logging.info(\\'train_acc %f\\', train_acc)\\n\\n    valid_acc_top1, valid_acc_top5, valid_obj = infer(valid_queue, model, criterion)\\n    logging.info(\\'valid_acc_top1 %f\\', valid_acc_top1)\\n    logging.info(\\'valid_acc_top5 %f\\', valid_acc_top5)\\n\\n    is_best = False\\n    if valid_acc_top1 > best_acc_top1:\\n      best_acc_top1 = valid_acc_top1\\n      is_best = True\\n\\n    utils.save_checkpoint({\\n      \\'epoch\\': epoch + 1,\\n      \\'state_dict\\': model.state_dict(),\\n      \\'best_acc_top1\\': best_acc_top1,\\n      \\'optimizer\\' : optimizer.state_dict(),\\n      }, is_best, args.save)\\n\\n\\ndef train(train_queue, model, criterion, optimizer):\\n  objs = utils.AvgrageMeter()\\n  top1 = utils.AvgrageMeter()\\n  top5 = utils.AvgrageMeter()\\n  model.train()\\n\\n  for step, (input, target) in enumerate(train_queue):\\n    target = target.cuda(async=True)\\n    input = input.cuda()\\n    input = Variable(input)\\n    target = Variable(target)\\n\\n    optimizer.zero_grad()\\n    logits, logits_aux = model(input)\\n    loss = criterion(logits, target)\\n    if args.auxiliary:\\n      loss_aux = criterion(logits_aux, target)\\n      loss += args.auxiliary_weight*loss_aux\\n\\n    loss.backward()\\n    nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\\n    optimizer.step()\\n\\n    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\\n    n = input.size(0)\\n    objs.update(loss.data[0], n)\\n    top1.update(prec1.data[0], n)\\n    top5.update(prec5.data[0], n)\\n\\n    if step % args.report_freq == 0:\\n      logging.info(\\'train %03d %e %f %f\\', step, objs.avg, top1.avg, top5.avg)\\n\\n  return top1.avg, objs.avg\\n\\n\\ndef infer(valid_queue, model, criterion):\\n  objs = utils.AvgrageMeter()\\n  top1 = utils.AvgrageMeter()\\n  top5 = utils.AvgrageMeter()\\n  model.eval()\\n\\n  for step, (input, target) in enumerate(valid_queue):\\n    input = Variable(input, volatile=True).cuda()\\n    target = Variable(target, volatile=True).cuda(async=True)\\n\\n    logits, _ = model(input)\\n    loss = criterion(logits, target)\\n\\n    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\\n    n = input.size(0)\\n    objs.update(loss.data[0], n)\\n    top1.update(prec1.data[0], n)\\n    top5.update(prec5.data[0], n)\\n\\n    if step % args.report_freq == 0:\\n      logging.info(\\'valid %03d %e %f %f\\', step, objs.avg, top1.avg, top5.avg)\\n\\n  return top1.avg, top5.avg, objs.avg\\n\\n\\nif __name__ == \\'__main__\\':\\n  main() \\nimport sys\\nimport genotypes\\nfrom graphviz import Digraph\\n\\n\\ndef plot(genotype, filename):\\n  g = Digraph(\\n      format=\\'pdf\\',\\n      edge_attr=dict(fontsize=\\'20\\', fontname=\"times\"),\\n      node_attr=dict(style=\\'filled\\', shape=\\'rect\\', align=\\'center\\', fontsize=\\'20\\', height=\\'0.5\\', width=\\'0.5\\', penwidth=\\'2\\', fontname=\"times\"),\\n      engine=\\'dot\\')\\n  g.body.extend([\\'rankdir=LR\\'])\\n\\n  g.node(\"x_{t}\", fillcolor=\\'darkseagreen2\\')\\n  g.node(\"h_{t-1}\", fillcolor=\\'darkseagreen2\\')\\n  g.node(\"0\", fillcolor=\\'lightblue\\')\\n  g.edge(\"x_{t}\", \"0\", fillcolor=\"gray\")\\n  g.edge(\"h_{t-1}\", \"0\", fillcolor=\"gray\")\\n  steps = len(genotype)\\n\\n  for i in range(1, steps + 1):\\n    g.node(str(i), fillcolor=\\'lightblue\\')\\n\\n  for i, (op, j) in enumerate(genotype):\\n    g.edge(str(j), str(i + 1), label=op, fillcolor=\"gray\")\\n\\n  g.node(\"h_{t}\", fillcolor=\\'palegoldenrod\\')\\n  for i in range(1, steps + 1):\\n    g.edge(str(i), \"h_{t}\", fillcolor=\"gray\")\\n\\n  g.render(filename, view=True)\\n\\n\\nif __name__ == \\'__main__\\':\\n  if len(sys.argv) != 2:\\n    print(\"usage:\\\\n python {} ARCH_NAME\".format(sys.argv[0]))\\n    sys.exit(1)\\n\\n  genotype_name = sys.argv[1]\\n  try:\\n    genotype = eval(\\'genotypes.{}\\'.format(genotype_name))\\n  except AttributeError:\\n    print(\"{} is not specified in genotypes.py\".format(genotype_name)) \\n    sys.exit(1)\\n\\n  plot(genotype.recurrent, \"recurrent\")\\n\\nimport argparse\\nimport os, sys\\nimport time\\nimport math\\nimport numpy as np\\nimport torch\\nimport torch.nn as nn\\nimport torch.backends.cudnn as cudnn\\n\\nimport data\\nimport model\\n\\nfrom utils import batchify, get_batch, repackage_hidden, create_exp_dir, save_checkpoint\\n\\nparser = argparse.ArgumentParser(description=\\'PyTorch PennTreeBank/WikiText2 Language Model\\')\\nparser.add_argument(\\'--data\\', type=str, default=\\'../data/penn/\\',\\n                    help=\\'location of the data corpus\\')\\nparser.add_argument(\\'--emsize\\', type=int, default=850,\\n                    help=\\'size of word embeddings\\')\\nparser.add_argument(\\'--nhid\\', type=int, default=850,\\n                    help=\\'number of hidden units per layer\\')\\nparser.add_argument(\\'--nhidlast\\', type=int, default=850,\\n                    help=\\'number of hidden units for the last rnn layer\\')\\nparser.add_argument(\\'--lr\\', type=float, default=20,\\n                    help=\\'initial learning rate\\')\\nparser.add_argument(\\'--clip\\', type=float, default=0.25,\\n                    help=\\'gradient clipping\\')\\nparser.add_argument(\\'--epochs\\', type=int, default=8000,\\n                    help=\\'upper epoch limit\\')\\nparser.add_argument(\\'--batch_size\\', type=int, default=64, metavar=\\'N\\',\\n                    help=\\'batch size\\')\\nparser.add_argument(\\'--bptt\\', type=int, default=35,\\n                    help=\\'sequence length\\')\\nparser.add_argument(\\'--dropout\\', type=float, default=0.75,\\n                    help=\\'dropout applied to layers (0 = no dropout)\\')\\nparser.add_argument(\\'--dropouth\\', type=float, default=0.3,\\n                    help=\\'dropout for rnn layers (0 = no dropout)\\')\\nparser.add_argument(\\'--dropouti\\', type=float, default=0.2,\\n                    help=\\'dropout for input embedding layers (0 = no dropout)\\')\\nparser.add_argument(\\'--dropoute\\', type=float, default=0.2,\\n                    help=\\'dropout to remove words from embedding layer (0 = no dropout)\\')\\nparser.add_argument(\\'--seed\\', type=int, default=1267,\\n                    help=\\'random seed\\')\\nparser.add_argument(\\'--nonmono\\', type=int, default=5,\\n                    help=\\'random seed\\')\\nparser.add_argument(\\'--cuda\\', action=\\'store_false\\',\\n                    help=\\'use CUDA\\')\\nparser.add_argument(\\'--log-interval\\', type=int, default=200, metavar=\\'N\\',\\n                    help=\\'report interval\\')\\nparser.add_argument(\\'--model_path\\', type=str,  default=\\'EXP/model.pt\\',\\n                    help=\\'path to load the pretrained model\\')\\nparser.add_argument(\\'--alpha\\', type=float, default=0,\\n                    help=\\'alpha L2 regularization on RNN activation (alpha = 0 means no regularization)\\')\\nparser.add_argument(\\'--beta\\', type=float, default=1e-3,\\n                    help=\\'beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\\')\\nparser.add_argument(\\'--wdecay\\', type=float, default=5e-7,\\n                    help=\\'weight decay applied to all weights\\')\\nparser.add_argument(\\'--continue_train\\', action=\\'store_true\\',\\n                    help=\\'continue train from a checkpoint\\')\\nparser.add_argument(\\'--n_experts\\', type=int, default=1,\\n                    help=\\'number of experts\\')\\nparser.add_argument(\\'--max_seq_len_delta\\', type=int, default=20,\\n                    help=\\'max sequence length\\')\\nparser.add_argument(\\'--gpu\\', type=int, default=0, help=\\'GPU device to use\\')\\nargs = parser.parse_args()\\n\\ndef logging(s, print_=True, log_=True):\\n    print(s)\\n\\n# Set the random seed manually for reproducibility.\\nnp.random.seed(args.seed)\\ntorch.manual_seed(args.seed)\\nif torch.cuda.is_available():\\n    if not args.cuda:\\n        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\\n    else:\\n        torch.cuda.set_device(args.gpu)\\n        cudnn.benchmark = True\\n        cudnn.enabled=True\\n        torch.cuda.manual_seed_all(args.seed)\\n\\n\\ncorpus = data.Corpus(args.data)\\ntest_batch_size = 1\\ntest_data = batchify(corpus.test, test_batch_size, args)\\n\\n\\ndef evaluate(data_source, batch_size=10):\\n    # Turn on evaluation mode which disables dropout.\\n    model.eval()\\n    total_loss = 0\\n    ntokens = len(corpus.dictionary)\\n    hidden = model.init_hidden(batch_size)\\n    for i in range(0, data_source.size(0) - 1, args.bptt):\\n        print(i, data_source.size(0)-1)\\n        data, targets = get_batch(data_source, i, args, evaluation=True)\\n        targets = targets.view(-1)\\n\\n        log_prob, hidden = parallel_model(data, hidden)\\n        loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), targets).data\\n\\n        total_loss += loss * len(data)\\n\\n        hidden = repackage_hidden(hidden)\\n    return total_loss[0] / len(data_source)\\n\\n# Load the best saved model.\\nmodel = torch.load(args.model_path)\\n\\ntotal_params = sum(x.data.nelement() for x in model.parameters())\\nlogging(\\'Args: {}\\'.format(args))\\nlogging(\\'Model total parameters: {}\\'.format(total_params))\\nparallel_model = model.cuda()\\n\\n# Run on test data.\\ntest_loss = evaluate(test_data, test_batch_size)\\nlogging(\\'=\\' * 89)\\nlogging(\\'| End of training | test loss {:5.2f} | test ppl {:8.2f}\\'.format(\\n    test_loss, math.exp(test_loss)))\\nlogging(\\'=\\' * 89)\\n\\nimport torch\\nimport numpy as np\\nimport torch.nn as nn\\nfrom torch.autograd import Variable\\n\\n\\ndef _concat(xs):\\n  return torch.cat([x.view(-1) for x in xs])\\n\\n\\ndef _clip(grads, max_norm):\\n    total_norm = 0\\n    for g in grads:\\n        param_norm = g.data.norm(2)\\n        total_norm += param_norm ** 2\\n    total_norm = total_norm ** 0.5\\n    clip_coef = max_norm / (total_norm + 1e-6)\\n    if clip_coef < 1:\\n        for g in grads:\\n            g.data.mul_(clip_coef)\\n    return clip_coef\\n\\n\\nclass Architect(object):\\n\\n  def __init__(self, model, args):\\n    self.network_weight_decay = args.wdecay\\n    self.network_clip = args.clip\\n    self.model = model\\n    self.optimizer = torch.optim.Adam(self.model.arch_parameters(), lr=args.arch_lr, weight_decay=args.arch_wdecay)\\n\\n  def _compute_unrolled_model(self, hidden, input, target, eta):\\n    loss, hidden_next = self.model._loss(hidden, input, target)\\n    theta = _concat(self.model.parameters()).data\\n    grads = torch.autograd.grad(loss, self.model.parameters())\\n    clip_coef = _clip(grads, self.network_clip)\\n    dtheta = _concat(grads).data + self.network_weight_decay*theta\\n    unrolled_model = self._construct_model_from_theta(theta.sub(eta, dtheta))\\n    return unrolled_model, clip_coef\\n\\n  def step(self,\\n          hidden_train, input_train, target_train,\\n          hidden_valid, input_valid, target_valid,\\n          network_optimizer, unrolled):\\n    eta = network_optimizer.param_groups[0][\\'lr\\']\\n    self.optimizer.zero_grad()\\n    if unrolled:\\n        hidden = self._backward_step_unrolled(hidden_train, input_train, target_train, hidden_valid, input_valid, target_valid, eta)\\n    else:\\n        hidden = self._backward_step(hidden_valid, input_valid, target_valid)\\n    self.optimizer.step()\\n    return hidden, None\\n\\n  def _backward_step(self, hidden, input, target):\\n    loss, hidden_next = self.model._loss(hidden, input, target)\\n    loss.backward()\\n    return hidden_next\\n\\n  def _backward_step_unrolled(self,\\n          hidden_train, input_train, target_train,\\n          hidden_valid, input_valid, target_valid, eta):\\n    unrolled_model, clip_coef = self._compute_unrolled_model(hidden_train, input_train, target_train, eta)\\n    unrolled_loss, hidden_next = unrolled_model._loss(hidden_valid, input_valid, target_valid)\\n\\n    unrolled_loss.backward()\\n    dalpha = [v.grad for v in unrolled_model.arch_parameters()]\\n    dtheta = [v.grad for v in unrolled_model.parameters()]\\n    _clip(dtheta, self.network_clip)\\n    vector = [dt.data for dt in dtheta]\\n    implicit_grads = self._hessian_vector_product(vector, hidden_train, input_train, target_train, r=1e-2)\\n\\n    for g, ig in zip(dalpha, implicit_grads):\\n      g.data.sub_(eta * clip_coef, ig.data)\\n\\n    for v, g in zip(self.model.arch_parameters(), dalpha):\\n      if v.grad is None:\\n        v.grad = Variable(g.data)\\n      else:\\n        v.grad.data.copy_(g.data)\\n    return hidden_next\\n\\n  def _construct_model_from_theta(self, theta):\\n    model_new = self.model.new()\\n    model_dict = self.model.state_dict()\\n\\n    params, offset = {}, 0\\n    for k, v in self.model.named_parameters():\\n      v_length = np.prod(v.size())\\n      params[k] = theta[offset: offset+v_length].view(v.size())\\n      offset += v_length\\n\\n    assert offset == len(theta)\\n    model_dict.update(params)\\n    model_new.load_state_dict(model_dict)\\n    return model_new.cuda()\\n\\n  def _hessian_vector_product(self, vector, hidden, input, target, r=1e-2):\\n    R = r / _concat(vector).norm()\\n    for p, v in zip(self.model.parameters(), vector):\\n      p.data.add_(R, v)\\n    loss, _ = self.model._loss(hidden, input, target)\\n    grads_p = torch.autograd.grad(loss, self.model.arch_parameters())\\n\\n    for p, v in zip(self.model.parameters(), vector):\\n      p.data.sub_(2*R, v)\\n    loss, _ = self.model._loss(hidden, input, target)\\n    grads_n = torch.autograd.grad(loss, self.model.arch_parameters())\\n\\n    for p, v in zip(self.model.parameters(), vector):\\n      p.data.add_(R, v)\\n\\n    return [(x-y).div_(2*R) for x, y in zip(grads_p, grads_n)]\\n\\nimport math\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom genotypes import STEPS\\nfrom utils import mask2d\\nfrom utils import LockedDropout\\nfrom utils import embedded_dropout\\nfrom torch.autograd import Variable\\n\\nINITRANGE = 0.04\\n\\n\\nclass DARTSCell(nn.Module):\\n\\n  def __init__(self, ninp, nhid, dropouth, dropoutx, genotype):\\n    super(DARTSCell, self).__init__()\\n    self.nhid = nhid\\n    self.dropouth = dropouth\\n    self.dropoutx = dropoutx\\n    self.genotype = genotype\\n\\n    # genotype is None when doing arch search\\n    steps = len(self.genotype.recurrent) if self.genotype is not None else STEPS\\n    self._W0 = nn.Parameter(torch.Tensor(ninp+nhid, 2*nhid).uniform_(-INITRANGE, INITRANGE))\\n    self._Ws = nn.ParameterList([\\n        nn.Parameter(torch.Tensor(nhid, 2*nhid).uniform_(-INITRANGE, INITRANGE)) for i in range(steps)\\n    ])\\n\\n  def forward(self, inputs, hidden):\\n    T, B = inputs.size(0), inputs.size(1)\\n\\n    if self.training:\\n      x_mask = mask2d(B, inputs.size(2), keep_prob=1.-self.dropoutx)\\n      h_mask = mask2d(B, hidden.size(2), keep_prob=1.-self.dropouth)\\n    else:\\n      x_mask = h_mask = None\\n\\n    hidden = hidden[0]\\n    hiddens = []\\n    for t in range(T):\\n      hidden = self.cell(inputs[t], hidden, x_mask, h_mask)\\n      hiddens.append(hidden)\\n    hiddens = torch.stack(hiddens)\\n    return hiddens, hiddens[-1].unsqueeze(0)\\n\\n  def _compute_init_state(self, x, h_prev, x_mask, h_mask):\\n    if self.training:\\n      xh_prev = torch.cat([x * x_mask, h_prev * h_mask], dim=-1)\\n    else:\\n      xh_prev = torch.cat([x, h_prev], dim=-1)\\n    c0, h0 = torch.split(xh_prev.mm(self._W0), self.nhid, dim=-1)\\n    c0 = c0.sigmoid()\\n    h0 = h0.tanh()\\n    s0 = h_prev + c0 * (h0-h_prev)\\n    return s0\\n\\n  def _get_activation(self, name):\\n    if name == \\'tanh\\':\\n      f = F.tanh\\n    elif name == \\'relu\\':\\n      f = F.relu\\n    elif name == \\'sigmoid\\':\\n      f = F.sigmoid\\n    elif name == \\'identity\\':\\n      f = lambda x: x\\n    else:\\n      raise NotImplementedError\\n    return f\\n\\n  def cell(self, x, h_prev, x_mask, h_mask):\\n    s0 = self._compute_init_state(x, h_prev, x_mask, h_mask)\\n\\n    states = [s0]\\n    for i, (name, pred) in enumerate(self.genotype.recurrent):\\n      s_prev = states[pred]\\n      if self.training:\\n        ch = (s_prev * h_mask).mm(self._Ws[i])\\n      else:\\n        ch = s_prev.mm(self._Ws[i])\\n      c, h = torch.split(ch, self.nhid, dim=-1)\\n      c = c.sigmoid()\\n      fn = self._get_activation(name)\\n      h = fn(h)\\n      s = s_prev + c * (h-s_prev)\\n      states += [s]\\n    output = torch.mean(torch.stack([states[i] for i in self.genotype.concat], -1), -1)\\n    return output\\n\\n\\nclass RNNModel(nn.Module):\\n    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\\n\\n    def __init__(self, ntoken, ninp, nhid, nhidlast, \\n                 dropout=0.5, dropouth=0.5, dropoutx=0.5, dropouti=0.5, dropoute=0.1,\\n                 cell_cls=DARTSCell, genotype=None):\\n        super(RNNModel, self).__init__()\\n        self.lockdrop = LockedDropout()\\n        self.encoder = nn.Embedding(ntoken, ninp)\\n        \\n        assert ninp == nhid == nhidlast\\n        if cell_cls == DARTSCell:\\n            assert genotype is not None\\n            self.rnns = [cell_cls(ninp, nhid, dropouth, dropoutx, genotype)]\\n        else:\\n            assert genotype is None\\n            self.rnns = [cell_cls(ninp, nhid, dropouth, dropoutx)]\\n\\n        self.rnns = torch.nn.ModuleList(self.rnns)\\n        self.decoder = nn.Linear(ninp, ntoken)\\n        self.decoder.weight = self.encoder.weight\\n        self.init_weights()\\n\\n        self.ninp = ninp\\n        self.nhid = nhid\\n        self.nhidlast = nhidlast\\n        self.dropout = dropout\\n        self.dropouti = dropouti\\n        self.dropoute = dropoute\\n        self.ntoken = ntoken\\n        self.cell_cls = cell_cls\\n\\n    def init_weights(self):\\n        self.encoder.weight.data.uniform_(-INITRANGE, INITRANGE)\\n        self.decoder.bias.data.fill_(0)\\n        self.decoder.weight.data.uniform_(-INITRANGE, INITRANGE)\\n\\n    def forward(self, input, hidden, return_h=False):\\n        batch_size = input.size(1)\\n\\n        emb = embedded_dropout(self.encoder, input, dropout=self.dropoute if self.training else 0)\\n        emb = self.lockdrop(emb, self.dropouti)\\n\\n        raw_output = emb\\n        new_hidden = []\\n        raw_outputs = []\\n        outputs = []\\n        for l, rnn in enumerate(self.rnns):\\n            current_input = raw_output\\n            raw_output, new_h = rnn(raw_output, hidden[l])\\n            new_hidden.append(new_h)\\n            raw_outputs.append(raw_output)\\n        hidden = new_hidden\\n\\n        output = self.lockdrop(raw_output, self.dropout)\\n        outputs.append(output)\\n\\n        logit = self.decoder(output.view(-1, self.ninp))\\n        log_prob = nn.functional.log_softmax(logit, dim=-1)\\n        model_output = log_prob\\n        model_output = model_output.view(-1, batch_size, self.ntoken)\\n\\n        if return_h:\\n            return model_output, hidden, raw_outputs, outputs\\n        return model_output, hidden\\n\\n    def init_hidden(self, bsz):\\n      weight = next(self.parameters()).data\\n      return [Variable(weight.new(1, bsz, self.nhid).zero_())]\\n\\nimport torch\\nimport torch.nn as nn\\nimport os, shutil\\nimport numpy as np\\nfrom torch.autograd import Variable\\n\\n\\ndef repackage_hidden(h):\\n    if type(h) == Variable:\\n        return Variable(h.data)\\n    else:\\n        return tuple(repackage_hidden(v) for v in h)\\n\\n\\ndef batchify(data, bsz, args):\\n    nbatch = data.size(0) // bsz\\n    data = data.narrow(0, 0, nbatch * bsz)\\n    data = data.view(bsz, -1).t().contiguous()\\n    print(data.size())\\n    if args.cuda:\\n        data = data.cuda()\\n    return data\\n\\n\\ndef get_batch(source, i, args, seq_len=None, evaluation=False):\\n    seq_len = min(seq_len if seq_len else args.bptt, len(source) - 1 - i)\\n    data = Variable(source[i:i+seq_len], volatile=evaluation)\\n    target = Variable(source[i+1:i+1+seq_len])\\n    return data, target\\n\\n\\ndef create_exp_dir(path, scripts_to_save=None):\\n    if not os.path.exists(path):\\n        os.mkdir(path)\\n\\n    print(\\'Experiment dir : {}\\'.format(path))\\n    if scripts_to_save is not None:\\n        os.mkdir(os.path.join(path, \\'scripts\\'))\\n        for script in scripts_to_save:\\n            dst_file = os.path.join(path, \\'scripts\\', os.path.basename(script))\\n            shutil.copyfile(script, dst_file)\\n\\n\\ndef save_checkpoint(model, optimizer, epoch, path, finetune=False):\\n    if finetune:\\n        torch.save(model, os.path.join(path, \\'finetune_model.pt\\'))\\n        torch.save(optimizer.state_dict(), os.path.join(path, \\'finetune_optimizer.pt\\'))\\n    else:\\n        torch.save(model, os.path.join(path, \\'model.pt\\'))\\n        torch.save(optimizer.state_dict(), os.path.join(path, \\'optimizer.pt\\'))\\n    torch.save({\\'epoch\\': epoch+1}, os.path.join(path, \\'misc.pt\\'))\\n\\n\\ndef embedded_dropout(embed, words, dropout=0.1, scale=None):\\n    if dropout:\\n        mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\\n        mask = Variable(mask)\\n        masked_embed_weight = mask * embed.weight\\n    else:\\n        masked_embed_weight = embed.weight\\n    if scale:\\n        masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\\n\\n    padding_idx = embed.padding_idx\\n    if padding_idx is None:\\n        padding_idx = -1\\n    X = embed._backend.Embedding.apply(words, masked_embed_weight,\\n        padding_idx, embed.max_norm, embed.norm_type,\\n        embed.scale_grad_by_freq, embed.sparse\\n    )\\n    return X\\n\\n\\nclass LockedDropout(nn.Module):\\n    def __init__(self):\\n        super(LockedDropout, self).__init__()\\n\\n    def forward(self, x, dropout=0.5):\\n        if not self.training or not dropout:\\n            return x\\n        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\\n        mask = Variable(m.div_(1 - dropout), requires_grad=False)\\n        mask = mask.expand_as(x)\\n        return mask * x\\n\\n\\ndef mask2d(B, D, keep_prob, cuda=True):\\n    m = torch.floor(torch.rand(B, D) + keep_prob) / keep_prob\\n    m = Variable(m, requires_grad=False)\\n    if cuda:\\n        m = m.cuda()\\n    return m\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom genotypes import PRIMITIVES, STEPS, CONCAT, Genotype\\nfrom torch.autograd import Variable\\nfrom collections import namedtuple\\nfrom model import DARTSCell, RNNModel\\n\\n\\nclass DARTSCellSearch(DARTSCell):\\n\\n  def __init__(self, ninp, nhid, dropouth, dropoutx):\\n    super(DARTSCellSearch, self).__init__(ninp, nhid, dropouth, dropoutx, genotype=None)\\n    self.bn = nn.BatchNorm1d(nhid, affine=False)\\n\\n  def cell(self, x, h_prev, x_mask, h_mask):\\n    s0 = self._compute_init_state(x, h_prev, x_mask, h_mask)\\n    s0 = self.bn(s0)\\n    probs = F.softmax(self.weights, dim=-1)\\n\\n    offset = 0\\n    states = s0.unsqueeze(0)\\n    for i in range(STEPS):\\n      if self.training:\\n        masked_states = states * h_mask.unsqueeze(0)\\n      else:\\n        masked_states = states\\n      ch = masked_states.view(-1, self.nhid).mm(self._Ws[i]).view(i+1, -1, 2*self.nhid)\\n      c, h = torch.split(ch, self.nhid, dim=-1)\\n      c = c.sigmoid()\\n\\n      s = torch.zeros_like(s0)\\n      for k, name in enumerate(PRIMITIVES):\\n        if name == \\'none\\':\\n          continue\\n        fn = self._get_activation(name)\\n        unweighted = states + c * (fn(h) - states)\\n        s += torch.sum(probs[offset:offset+i+1, k].unsqueeze(-1).unsqueeze(-1) * unweighted, dim=0)\\n      s = self.bn(s)\\n      states = torch.cat([states, s.unsqueeze(0)], 0)\\n      offset += i+1\\n    output = torch.mean(states[-CONCAT:], dim=0)\\n    return output\\n\\n\\nclass RNNModelSearch(RNNModel):\\n\\n    def __init__(self, *args):\\n        super(RNNModelSearch, self).__init__(*args, cell_cls=DARTSCellSearch, genotype=None)\\n        self._args = args\\n        self._initialize_arch_parameters()\\n\\n    def new(self):\\n        model_new = RNNModelSearch(*self._args)\\n        for x, y in zip(model_new.arch_parameters(), self.arch_parameters()):\\n            x.data.copy_(y.data)\\n        return model_new\\n\\n    def _initialize_arch_parameters(self):\\n      k = sum(i for i in range(1, STEPS+1))\\n      weights_data = torch.randn(k, len(PRIMITIVES)).mul_(1e-3)\\n      self.weights = Variable(weights_data.cuda(), requires_grad=True)\\n      self._arch_parameters = [self.weights]\\n      for rnn in self.rnns:\\n        rnn.weights = self.weights\\n\\n    def arch_parameters(self):\\n      return self._arch_parameters\\n\\n    def _loss(self, hidden, input, target):\\n      log_prob, hidden_next = self(input, hidden, return_h=False)\\n      loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), target)\\n      return loss, hidden_next\\n\\n    def genotype(self):\\n\\n      def _parse(probs):\\n        gene = []\\n        start = 0\\n        for i in range(STEPS):\\n          end = start + i + 1\\n          W = probs[start:end].copy()\\n          j = sorted(range(i + 1), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index(\\'none\\')))[0]\\n          k_best = None\\n          for k in range(len(W[j])):\\n            if k != PRIMITIVES.index(\\'none\\'):\\n              if k_best is None or W[j][k] > W[j][k_best]:\\n                k_best = k\\n          gene.append((PRIMITIVES[k_best], j))\\n          start = end\\n        return gene\\n\\n      gene = _parse(F.softmax(self.weights, dim=-1).data.cpu().numpy())\\n      genotype = Genotype(recurrent=gene, concat=range(STEPS+1)[-CONCAT:])\\n      return genotype\\n\\nimport os\\nimport gc\\nimport sys\\nimport glob\\nimport time\\nimport math\\nimport numpy as np\\nimport torch\\nimport torch.nn as nn\\nimport logging\\nimport argparse\\nimport genotypes\\nimport torch.nn.functional as F\\nimport torch.backends.cudnn as cudnn\\nimport data\\nimport model\\n\\nfrom torch.autograd import Variable\\nfrom utils import batchify, get_batch, repackage_hidden, create_exp_dir, save_checkpoint\\n\\nparser = argparse.ArgumentParser(description=\\'PyTorch PennTreeBank/WikiText2 Language Model\\')\\nparser.add_argument(\\'--data\\', type=str, default=\\'../data/penn/\\',\\n                    help=\\'location of the data corpus\\')\\nparser.add_argument(\\'--emsize\\', type=int, default=850,\\n                    help=\\'size of word embeddings\\')\\nparser.add_argument(\\'--nhid\\', type=int, default=850,\\n                    help=\\'number of hidden units per layer\\')\\nparser.add_argument(\\'--nhidlast\\', type=int, default=850,\\n                    help=\\'number of hidden units for the last rnn layer\\')\\nparser.add_argument(\\'--lr\\', type=float, default=20,\\n                    help=\\'initial learning rate\\')\\nparser.add_argument(\\'--clip\\', type=float, default=0.25,\\n                    help=\\'gradient clipping\\')\\nparser.add_argument(\\'--epochs\\', type=int, default=8000,\\n                    help=\\'upper epoch limit\\')\\nparser.add_argument(\\'--batch_size\\', type=int, default=64, metavar=\\'N\\',\\n                    help=\\'batch size\\')\\nparser.add_argument(\\'--bptt\\', type=int, default=35,\\n                    help=\\'sequence length\\')\\nparser.add_argument(\\'--dropout\\', type=float, default=0.75,\\n                    help=\\'dropout applied to layers (0 = no dropout)\\')\\nparser.add_argument(\\'--dropouth\\', type=float, default=0.25,\\n                    help=\\'dropout for hidden nodes in rnn layers (0 = no dropout)\\')\\nparser.add_argument(\\'--dropoutx\\', type=float, default=0.75,\\n                    help=\\'dropout for input nodes rnn layers (0 = no dropout)\\')\\nparser.add_argument(\\'--dropouti\\', type=float, default=0.2,\\n                    help=\\'dropout for input embedding layers (0 = no dropout)\\')\\nparser.add_argument(\\'--dropoute\\', type=float, default=0.1,\\n                    help=\\'dropout to remove words from embedding layer (0 = no dropout)\\')\\nparser.add_argument(\\'--seed\\', type=int, default=1267,\\n                    help=\\'random seed\\')\\nparser.add_argument(\\'--nonmono\\', type=int, default=5,\\n                    help=\\'random seed\\')\\nparser.add_argument(\\'--cuda\\', action=\\'store_false\\',\\n                    help=\\'use CUDA\\')\\nparser.add_argument(\\'--log-interval\\', type=int, default=200, metavar=\\'N\\',\\n                    help=\\'report interval\\')\\nparser.add_argument(\\'--save\\', type=str,  default=\\'EXP\\',\\n                    help=\\'path to save the final model\\')\\nparser.add_argument(\\'--alpha\\', type=float, default=0,\\n                    help=\\'alpha L2 regularization on RNN activation (alpha = 0 means no regularization)\\')\\nparser.add_argument(\\'--beta\\', type=float, default=1e-3,\\n                    help=\\'beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\\')\\nparser.add_argument(\\'--wdecay\\', type=float, default=8e-7,\\n                    help=\\'weight decay applied to all weights\\')\\nparser.add_argument(\\'--continue_train\\', action=\\'store_true\\',\\n                    help=\\'continue train from a checkpoint\\')\\nparser.add_argument(\\'--small_batch_size\\', type=int, default=-1,\\n                    help=\\'the batch size for computation. batch_size should be divisible by small_batch_size.\\\\\\n                     In our implementation, we compute gradients with small_batch_size multiple times, and accumulate the gradients\\\\\\n                     until batch_size is reached. An update step is then performed.\\')\\nparser.add_argument(\\'--max_seq_len_delta\\', type=int, default=20,\\n                    help=\\'max sequence length\\')\\nparser.add_argument(\\'--single_gpu\\', default=True, action=\\'store_false\\', \\n                    help=\\'use single GPU\\')\\nparser.add_argument(\\'--gpu\\', type=int, default=0, help=\\'GPU device to use\\')\\nparser.add_argument(\\'--arch\\', type=str, default=\\'DARTS\\', help=\\'which architecture to use\\')\\nargs = parser.parse_args()\\n\\nif args.nhidlast < 0:\\n    args.nhidlast = args.emsize\\nif args.small_batch_size < 0:\\n    args.small_batch_size = args.batch_size\\n\\nif not args.continue_train:\\n    args.save = \\'eval-{}-{}\\'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\\n    create_exp_dir(args.save, scripts_to_save=glob.glob(\\'*.py\\'))\\n\\nlog_format = \\'%(asctime)s %(message)s\\'\\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO,\\n    format=log_format, datefmt=\\'%m/%d %I:%M:%S %p\\')\\nfh = logging.FileHandler(os.path.join(args.save, \\'log.txt\\'))\\nfh.setFormatter(logging.Formatter(log_format))\\nlogging.getLogger().addHandler(fh)\\n\\n# Set the random seed manually for reproducibility.\\nnp.random.seed(args.seed)\\ntorch.manual_seed(args.seed)\\nif torch.cuda.is_available():\\n    if not args.cuda:\\n        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\\n    else:\\n        torch.cuda.set_device(args.gpu)\\n        cudnn.benchmark = True\\n        cudnn.enabled=True\\n        torch.cuda.manual_seed_all(args.seed)\\n\\ncorpus = data.Corpus(args.data)\\n\\neval_batch_size = 10\\ntest_batch_size = 1\\ntrain_data = batchify(corpus.train, args.batch_size, args)\\nval_data = batchify(corpus.valid, eval_batch_size, args)\\ntest_data = batchify(corpus.test, test_batch_size, args)\\n\\n\\nntokens = len(corpus.dictionary)\\nif args.continue_train:\\n    model = torch.load(os.path.join(args.save, \\'model.pt\\'))\\nelse:\\n    genotype = eval(\"genotypes.%s\" % args.arch)\\n    model = model.RNNModel(ntokens, args.emsize, args.nhid, args.nhidlast, \\n                       args.dropout, args.dropouth, args.dropoutx, args.dropouti, args.dropoute, \\n                       cell_cls=model.DARTSCell, genotype=genotype)\\n\\nif args.cuda:\\n    if args.single_gpu:\\n        parallel_model = model.cuda()\\n    else:\\n        parallel_model = nn.DataParallel(model, dim=1).cuda()\\nelse:\\n    parallel_model = model\\n\\ntotal_params = sum(x.data.nelement() for x in model.parameters())\\nlogging.info(\\'Args: {}\\'.format(args))\\nlogging.info(\\'Model total parameters: {}\\'.format(total_params))\\nlogging.info(\\'Genotype: {}\\'.format(genotype))\\n\\n\\ndef evaluate(data_source, batch_size=10):\\n    # Turn on evaluation mode which disables dropout.\\n    model.eval()\\n    total_loss = 0\\n    ntokens = len(corpus.dictionary)\\n    hidden = model.init_hidden(batch_size)\\n    for i in range(0, data_source.size(0) - 1, args.bptt):\\n        data, targets = get_batch(data_source, i, args, evaluation=True)\\n        targets = targets.view(-1)\\n\\n        log_prob, hidden = parallel_model(data, hidden)\\n        loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), targets).data\\n\\n        total_loss += loss * len(data)\\n\\n        hidden = repackage_hidden(hidden)\\n    return total_loss[0] / len(data_source)\\n\\n\\ndef train():\\n    assert args.batch_size % args.small_batch_size == 0, \\'batch_size must be divisible by small_batch_size\\'\\n\\n    # Turn on training mode which enables dropout.\\n    total_loss = 0\\n    start_time = time.time()\\n    ntokens = len(corpus.dictionary)\\n    hidden = [model.init_hidden(args.small_batch_size) for _ in range(args.batch_size // args.small_batch_size)]\\n    batch, i = 0, 0\\n    while i < train_data.size(0) - 1 - 1:\\n        bptt = args.bptt if np.random.random() < 0.95 else args.bptt / 2.\\n        # Prevent excessively small or negative sequence lengths\\n        seq_len = max(5, int(np.random.normal(bptt, 5)))\\n        # There\\'s a very small chance that it could select a very long sequence length resulting in OOM\\n        seq_len = min(seq_len, args.bptt + args.max_seq_len_delta)\\n\\n        lr2 = optimizer.param_groups[0][\\'lr\\']\\n        optimizer.param_groups[0][\\'lr\\'] = lr2 * seq_len / args.bptt\\n        model.train()\\n        data, targets = get_batch(train_data, i, args, seq_len=seq_len)\\n\\n        optimizer.zero_grad()\\n\\n        start, end, s_id = 0, args.small_batch_size, 0\\n        while start < args.batch_size:\\n            cur_data, cur_targets = data[:, start: end], targets[:, start: end].contiguous().view(-1)\\n\\n            # Starting each batch, we detach the hidden state from how it was previously produced.\\n            # If we didn\\'t, the model would try backpropagating all the way to start of the dataset.\\n            hidden[s_id] = repackage_hidden(hidden[s_id])\\n\\n            log_prob, hidden[s_id], rnn_hs, dropped_rnn_hs = parallel_model(cur_data, hidden[s_id], return_h=True)\\n            raw_loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), cur_targets)\\n\\n            loss = raw_loss\\n            # Activiation Regularization\\n            if args.alpha > 0:\\n              loss = loss + sum(args.alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\\n            # Temporal Activation Regularization (slowness)\\n            loss = loss + sum(args.beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\\n            loss *= args.small_batch_size / args.batch_size\\n            total_loss += raw_loss.data * args.small_batch_size / args.batch_size\\n            loss.backward()\\n\\n            s_id += 1\\n            start = end\\n            end = start + args.small_batch_size\\n\\n            gc.collect()\\n\\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs.\\n        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\\n        optimizer.step()\\n\\n        # total_loss += raw_loss.data\\n        optimizer.param_groups[0][\\'lr\\'] = lr2\\n\\n        if np.isnan(total_loss[0]):\\n          raise\\n\\n        if batch % args.log_interval == 0 and batch > 0:\\n            cur_loss = total_loss[0] / args.log_interval\\n            elapsed = time.time() - start_time\\n            logging.info(\\'| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | \\'\\n                    \\'loss {:5.2f} | ppl {:8.2f}\\'.format(\\n                epoch, batch, len(train_data) // args.bptt, optimizer.param_groups[0][\\'lr\\'],\\n                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\\n            total_loss = 0\\n            start_time = time.time()\\n        batch += 1\\n        i += seq_len\\n\\n# Loop over epochs.\\nlr = args.lr\\nbest_val_loss = []\\nstored_loss = 100000000\\n\\n# At any point you can hit Ctrl + C to break out of training early.\\ntry:\\n    if args.continue_train:\\n        optimizer_state = torch.load(os.path.join(args.save, \\'optimizer.pt\\'))\\n        if \\'t0\\' in optimizer_state[\\'param_groups\\'][0]:\\n            optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\\n        else:\\n            optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\\n        optimizer.load_state_dict(optimizer_state)\\n    else:\\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\\n\\n    epoch = 1\\n    while epoch < args.epochs + 1:\\n        epoch_start_time = time.time()\\n        try:\\n          train()\\n        except:\\n          logging.info(\\'rolling back to the previous best model ...\\')\\n          model = torch.load(os.path.join(args.save, \\'model.pt\\'))\\n          parallel_model = model.cuda()\\n          \\n          optimizer_state = torch.load(os.path.join(args.save, \\'optimizer.pt\\'))\\n          if \\'t0\\' in optimizer_state[\\'param_groups\\'][0]:\\n            optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\\n          else:\\n            optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\\n          optimizer.load_state_dict(optimizer_state)\\n\\n          epoch = torch.load(os.path.join(args.save, \\'misc.pt\\'))[\\'epoch\\']\\n          continue\\n\\n        if \\'t0\\' in optimizer.param_groups[0]:\\n            tmp = {}\\n            for prm in model.parameters():\\n                tmp[prm] = prm.data.clone()\\n                prm.data = optimizer.state[prm][\\'ax\\'].clone()\\n\\n            val_loss2 = evaluate(val_data)\\n            logging.info(\\'-\\' * 89)\\n            logging.info(\\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | \\'\\n                    \\'valid ppl {:8.2f}\\'.format(epoch, (time.time() - epoch_start_time),\\n                                               val_loss2, math.exp(val_loss2)))\\n            logging.info(\\'-\\' * 89)\\n\\n            if val_loss2 < stored_loss:\\n                save_checkpoint(model, optimizer, epoch, args.save)\\n                logging.info(\\'Saving Averaged!\\')\\n                stored_loss = val_loss2\\n\\n            for prm in model.parameters():\\n                prm.data = tmp[prm].clone()\\n\\n        else:\\n            val_loss = evaluate(val_data, eval_batch_size)\\n            logging.info(\\'-\\' * 89)\\n            logging.info(\\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | \\'\\n                    \\'valid ppl {:8.2f}\\'.format(epoch, (time.time() - epoch_start_time),\\n                                               val_loss, math.exp(val_loss)))\\n            logging.info(\\'-\\' * 89)\\n\\n            if val_loss < stored_loss:\\n                save_checkpoint(model, optimizer, epoch, args.save)\\n                logging.info(\\'Saving Normal!\\')\\n                stored_loss = val_loss\\n\\n            if \\'t0\\' not in optimizer.param_groups[0] and (len(best_val_loss)>args.nonmono and val_loss > min(best_val_loss[:-args.nonmono])):\\n                logging.info(\\'Switching!\\')\\n                optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\\n            best_val_loss.append(val_loss)\\n\\n        epoch += 1\\n\\nexcept KeyboardInterrupt:\\n    logging.info(\\'-\\' * 89)\\n    logging.info(\\'Exiting from training early\\')\\n\\n# Load the best saved model.\\nmodel = torch.load(os.path.join(args.save, \\'model.pt\\'))\\nparallel_model = model.cuda()\\n\\n# Run on test data.\\ntest_loss = evaluate(test_data, test_batch_size)\\nlogging.info(\\'=\\' * 89)\\nlogging.info(\\'| End of training | test loss {:5.2f} | test ppl {:8.2f}\\'.format(\\n    test_loss, math.exp(test_loss)))\\nlogging.info(\\'=\\' * 89)\\nfrom collections import namedtuple\\n\\nGenotype = namedtuple(\\'Genotype\\', \\'recurrent concat\\')\\n\\nPRIMITIVES = [\\n    \\'none\\',\\n    \\'tanh\\',\\n    \\'relu\\',\\n    \\'sigmoid\\',\\n    \\'identity\\'\\n]\\nSTEPS = 8\\nCONCAT = 8\\n\\nENAS = Genotype(\\n    recurrent = [\\n        (\\'tanh\\', 0),\\n        (\\'tanh\\', 1),\\n        (\\'relu\\', 1),\\n        (\\'tanh\\', 3),\\n        (\\'tanh\\', 3),\\n        (\\'relu\\', 3),\\n        (\\'relu\\', 4),\\n        (\\'relu\\', 7),\\n        (\\'relu\\', 8),\\n        (\\'relu\\', 8),\\n        (\\'relu\\', 8),\\n    ],\\n    concat = [2, 5, 6, 9, 10, 11]\\n)\\n\\nDARTS_V1 = Genotype(recurrent=[(\\'relu\\', 0), (\\'relu\\', 1), (\\'tanh\\', 2), (\\'relu\\', 3), (\\'relu\\', 4), (\\'identity\\', 1), (\\'relu\\', 5), (\\'relu\\', 1)], concat=range(1, 9))\\nDARTS_V2 = Genotype(recurrent=[(\\'sigmoid\\', 0), (\\'relu\\', 1), (\\'relu\\', 1), (\\'identity\\', 1), (\\'tanh\\', 2), (\\'sigmoid\\', 5), (\\'tanh\\', 3), (\\'relu\\', 5)], concat=range(1, 9))\\n\\nDARTS = DARTS_V2\\n\\nimport argparse\\nimport os, sys, glob\\nimport time\\nimport math\\nimport numpy as np\\nimport torch\\nimport logging\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.backends.cudnn as cudnn\\nfrom architect import Architect\\n\\nimport gc\\n\\nimport data\\nimport model_search as model\\n\\nfrom utils import batchify, get_batch, repackage_hidden, create_exp_dir, save_checkpoint\\n\\nparser = argparse.ArgumentParser(description=\\'PyTorch PennTreeBank/WikiText2 Language Model\\')\\nparser.add_argument(\\'--data\\', type=str, default=\\'../data/penn/\\',\\n                    help=\\'location of the data corpus\\')\\nparser.add_argument(\\'--emsize\\', type=int, default=300,\\n                    help=\\'size of word embeddings\\')\\nparser.add_argument(\\'--nhid\\', type=int, default=300,\\n                    help=\\'number of hidden units per layer\\')\\nparser.add_argument(\\'--nhidlast\\', type=int, default=300,\\n                    help=\\'number of hidden units for the last rnn layer\\')\\nparser.add_argument(\\'--lr\\', type=float, default=20,\\n                    help=\\'initial learning rate\\')\\nparser.add_argument(\\'--clip\\', type=float, default=0.25,\\n                    help=\\'gradient clipping\\')\\nparser.add_argument(\\'--epochs\\', type=int, default=50,\\n                    help=\\'upper epoch limit\\')\\nparser.add_argument(\\'--batch_size\\', type=int, default=256, metavar=\\'N\\',\\n                    help=\\'batch size\\')\\nparser.add_argument(\\'--bptt\\', type=int, default=35,\\n                    help=\\'sequence length\\')\\nparser.add_argument(\\'--dropout\\', type=float, default=0.75,\\n                    help=\\'dropout applied to layers (0 = no dropout)\\')\\nparser.add_argument(\\'--dropouth\\', type=float, default=0.25,\\n                    help=\\'dropout for hidden nodes in rnn layers (0 = no dropout)\\')\\nparser.add_argument(\\'--dropoutx\\', type=float, default=0.75,\\n                    help=\\'dropout for input nodes in rnn layers (0 = no dropout)\\')\\nparser.add_argument(\\'--dropouti\\', type=float, default=0.2,\\n                    help=\\'dropout for input embedding layers (0 = no dropout)\\')\\nparser.add_argument(\\'--dropoute\\', type=float, default=0,\\n                    help=\\'dropout to remove words from embedding layer (0 = no dropout)\\')\\nparser.add_argument(\\'--seed\\', type=int, default=3,\\n                    help=\\'random seed\\')\\nparser.add_argument(\\'--nonmono\\', type=int, default=5,\\n                    help=\\'random seed\\')\\nparser.add_argument(\\'--cuda\\', action=\\'store_false\\',\\n                    help=\\'use CUDA\\')\\nparser.add_argument(\\'--log-interval\\', type=int, default=50, metavar=\\'N\\',\\n                    help=\\'report interval\\')\\nparser.add_argument(\\'--save\\', type=str,  default=\\'EXP\\',\\n                    help=\\'path to save the final model\\')\\nparser.add_argument(\\'--alpha\\', type=float, default=0,\\n                    help=\\'alpha L2 regularization on RNN activation (alpha = 0 means no regularization)\\')\\nparser.add_argument(\\'--beta\\', type=float, default=1e-3,\\n                    help=\\'beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\\')\\nparser.add_argument(\\'--wdecay\\', type=float, default=5e-7,\\n                    help=\\'weight decay applied to all weights\\')\\nparser.add_argument(\\'--continue_train\\', action=\\'store_true\\',\\n                    help=\\'continue train from a checkpoint\\')\\nparser.add_argument(\\'--small_batch_size\\', type=int, default=-1,\\n                    help=\\'the batch size for computation. batch_size should be divisible by small_batch_size.\\\\\\n                     In our implementation, we compute gradients with small_batch_size multiple times, and accumulate the gradients\\\\\\n                     until batch_size is reached. An update step is then performed.\\')\\nparser.add_argument(\\'--max_seq_len_delta\\', type=int, default=20,\\n                    help=\\'max sequence length\\')\\nparser.add_argument(\\'--single_gpu\\', default=True, action=\\'store_false\\', \\n                    help=\\'use single GPU\\')\\nparser.add_argument(\\'--gpu\\', type=int, default=0, help=\\'GPU device to use\\')\\nparser.add_argument(\\'--unrolled\\', action=\\'store_true\\', default=False, help=\\'use one-step unrolled validation loss\\')\\nparser.add_argument(\\'--arch_wdecay\\', type=float, default=1e-3,\\n                    help=\\'weight decay for the architecture encoding alpha\\')\\nparser.add_argument(\\'--arch_lr\\', type=float, default=3e-3,\\n                    help=\\'learning rate for the architecture encoding alpha\\')\\nargs = parser.parse_args()\\n\\nif args.nhidlast < 0:\\n    args.nhidlast = args.emsize\\nif args.small_batch_size < 0:\\n    args.small_batch_size = args.batch_size\\n\\nif not args.continue_train:\\n    args.save = \\'search-{}-{}\\'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\\n    create_exp_dir(args.save, scripts_to_save=glob.glob(\\'*.py\\'))\\n\\nlog_format = \\'%(asctime)s %(message)s\\'\\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO,\\n    format=log_format, datefmt=\\'%m/%d %I:%M:%S %p\\')\\nfh = logging.FileHandler(os.path.join(args.save, \\'log.txt\\'))\\nfh.setFormatter(logging.Formatter(log_format))\\nlogging.getLogger().addHandler(fh)\\n\\n# Set the random seed manually for reproducibility.\\nnp.random.seed(args.seed)\\ntorch.manual_seed(args.seed)\\nif torch.cuda.is_available():\\n    if not args.cuda:\\n        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\\n    else:\\n        torch.cuda.set_device(args.gpu)\\n        cudnn.benchmark = True\\n        cudnn.enabled=True\\n        torch.cuda.manual_seed_all(args.seed)\\n\\ncorpus = data.Corpus(args.data)\\n\\neval_batch_size = 10\\ntest_batch_size = 1\\n\\ntrain_data = batchify(corpus.train, args.batch_size, args)\\nsearch_data = batchify(corpus.valid, args.batch_size, args)\\nval_data = batchify(corpus.valid, eval_batch_size, args)\\ntest_data = batchify(corpus.test, test_batch_size, args)\\n\\n\\nntokens = len(corpus.dictionary)\\nif args.continue_train:\\n    model = torch.load(os.path.join(args.save, \\'model.pt\\'))\\nelse:\\n    model = model.RNNModelSearch(ntokens, args.emsize, args.nhid, args.nhidlast, \\n                       args.dropout, args.dropouth, args.dropoutx, args.dropouti, args.dropoute)\\n\\nsize = 0\\nfor p in model.parameters():\\n    size += p.nelement()\\nlogging.info(\\'param size: {}\\'.format(size))\\nlogging.info(\\'initial genotype:\\')\\nlogging.info(model.genotype())\\n\\nif args.cuda:\\n    if args.single_gpu:\\n        parallel_model = model.cuda()\\n    else:\\n        parallel_model = nn.DataParallel(model, dim=1).cuda()\\nelse:\\n    parallel_model = model\\narchitect = Architect(parallel_model, args)\\n\\ntotal_params = sum(x.data.nelement() for x in model.parameters())\\nlogging.info(\\'Args: {}\\'.format(args))\\nlogging.info(\\'Model total parameters: {}\\'.format(total_params))\\n\\n\\ndef evaluate(data_source, batch_size=10):\\n    # Turn on evaluation mode which disables dropout.\\n    model.eval()\\n    total_loss = 0\\n    ntokens = len(corpus.dictionary)\\n    hidden = model.init_hidden(batch_size)\\n    for i in range(0, data_source.size(0) - 1, args.bptt):\\n        data, targets = get_batch(data_source, i, args, evaluation=True)\\n        targets = targets.view(-1)\\n\\n        log_prob, hidden = parallel_model(data, hidden)\\n        loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), targets).data\\n\\n        total_loss += loss * len(data)\\n\\n        hidden = repackage_hidden(hidden)\\n    return total_loss[0] / len(data_source)\\n\\n\\ndef train():\\n    assert args.batch_size % args.small_batch_size == 0, \\'batch_size must be divisible by small_batch_size\\'\\n\\n    # Turn on training mode which enables dropout.\\n    total_loss = 0\\n    start_time = time.time()\\n    ntokens = len(corpus.dictionary)\\n    hidden = [model.init_hidden(args.small_batch_size) for _ in range(args.batch_size // args.small_batch_size)]\\n    hidden_valid = [model.init_hidden(args.small_batch_size) for _ in range(args.batch_size // args.small_batch_size)]\\n    batch, i = 0, 0\\n    while i < train_data.size(0) - 1 - 1:\\n        bptt = args.bptt if np.random.random() < 0.95 else args.bptt / 2.\\n        # Prevent excessively small or negative sequence lengths\\n        # seq_len = max(5, int(np.random.normal(bptt, 5)))\\n        # # There\\'s a very small chance that it could select a very long sequence length resulting in OOM\\n        # seq_len = min(seq_len, args.bptt + args.max_seq_len_delta)\\n        seq_len = int(bptt)\\n\\n        lr2 = optimizer.param_groups[0][\\'lr\\']\\n        optimizer.param_groups[0][\\'lr\\'] = lr2 * seq_len / args.bptt\\n        model.train()\\n\\n        data_valid, targets_valid = get_batch(search_data, i % (search_data.size(0) - 1), args)\\n        data, targets = get_batch(train_data, i, args, seq_len=seq_len)\\n\\n        optimizer.zero_grad()\\n\\n        start, end, s_id = 0, args.small_batch_size, 0\\n        while start < args.batch_size:\\n            cur_data, cur_targets = data[:, start: end], targets[:, start: end].contiguous().view(-1)\\n            cur_data_valid, cur_targets_valid = data_valid[:, start: end], targets_valid[:, start: end].contiguous().view(-1)\\n\\n            # Starting each batch, we detach the hidden state from how it was previously produced.\\n            # If we didn\\'t, the model would try backpropagating all the way to start of the dataset.\\n            hidden[s_id] = repackage_hidden(hidden[s_id])\\n            hidden_valid[s_id] = repackage_hidden(hidden_valid[s_id])\\n\\n            hidden_valid[s_id], grad_norm = architect.step(\\n                    hidden[s_id], cur_data, cur_targets,\\n                    hidden_valid[s_id], cur_data_valid, cur_targets_valid,\\n                    optimizer,\\n                    args.unrolled)\\n\\n            # assuming small_batch_size = batch_size so we don\\'t accumulate gradients\\n            optimizer.zero_grad()\\n            hidden[s_id] = repackage_hidden(hidden[s_id])\\n\\n            log_prob, hidden[s_id], rnn_hs, dropped_rnn_hs = parallel_model(cur_data, hidden[s_id], return_h=True)\\n            raw_loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), cur_targets)\\n\\n            loss = raw_loss\\n            # Activiation Regularization\\n            if args.alpha > 0:\\n              loss = loss + sum(args.alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\\n            # Temporal Activation Regularization (slowness)\\n            loss = loss + sum(args.beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\\n            loss *= args.small_batch_size / args.batch_size\\n            total_loss += raw_loss.data * args.small_batch_size / args.batch_size\\n            loss.backward()\\n\\n            s_id += 1\\n            start = end\\n            end = start + args.small_batch_size\\n\\n            gc.collect()\\n\\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs.\\n        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\\n        optimizer.step()\\n\\n        # total_loss += raw_loss.data\\n        optimizer.param_groups[0][\\'lr\\'] = lr2\\n        if batch % args.log_interval == 0 and batch > 0:\\n            logging.info(parallel_model.genotype())\\n            print(F.softmax(parallel_model.weights, dim=-1))\\n            cur_loss = total_loss[0] / args.log_interval\\n            elapsed = time.time() - start_time\\n            logging.info(\\'| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | \\'\\n                    \\'loss {:5.2f} | ppl {:8.2f}\\'.format(\\n                epoch, batch, len(train_data) // args.bptt, optimizer.param_groups[0][\\'lr\\'],\\n                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\\n            total_loss = 0\\n            start_time = time.time()\\n        batch += 1\\n        i += seq_len\\n\\n# Loop over epochs.\\nlr = args.lr\\nbest_val_loss = []\\nstored_loss = 100000000\\n\\nif args.continue_train:\\n    optimizer_state = torch.load(os.path.join(args.save, \\'optimizer.pt\\'))\\n    if \\'t0\\' in optimizer_state[\\'param_groups\\'][0]:\\n        optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\\n    else:\\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\\n    optimizer.load_state_dict(optimizer_state)\\nelse:\\n    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\\n\\nfor epoch in range(1, args.epochs+1):\\n    epoch_start_time = time.time()\\n    train()\\n\\n    val_loss = evaluate(val_data, eval_batch_size)\\n    logging.info(\\'-\\' * 89)\\n    logging.info(\\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | \\'\\n            \\'valid ppl {:8.2f}\\'.format(epoch, (time.time() - epoch_start_time),\\n                                       val_loss, math.exp(val_loss)))\\n    logging.info(\\'-\\' * 89)\\n\\n    if val_loss < stored_loss:\\n        save_checkpoint(model, optimizer, epoch, args.save)\\n        logging.info(\\'Saving Normal!\\')\\n        stored_loss = val_loss\\n\\n    best_val_loss.append(val_loss)\\nimport os\\nimport torch\\n\\nfrom collections import Counter\\n\\n\\nclass Dictionary(object):\\n    def __init__(self):\\n        self.word2idx = {}\\n        self.idx2word = []\\n        self.counter = Counter()\\n        self.total = 0\\n\\n    def add_word(self, word):\\n        if word not in self.word2idx:\\n            self.idx2word.append(word)\\n            self.word2idx[word] = len(self.idx2word) - 1\\n        token_id = self.word2idx[word]\\n        self.counter[token_id] += 1\\n        self.total += 1\\n        return self.word2idx[word]\\n\\n    def __len__(self):\\n        return len(self.idx2word)\\n\\n\\nclass Corpus(object):\\n    def __init__(self, path):\\n        self.dictionary = Dictionary()\\n        self.train = self.tokenize(os.path.join(path, \\'train.txt\\'))\\n        self.valid = self.tokenize(os.path.join(path, \\'valid.txt\\'))\\n        self.test = self.tokenize(os.path.join(path, \\'test.txt\\'))\\n\\n    def tokenize(self, path):\\n        \"\"\"Tokenizes a text file.\"\"\"\\n        assert os.path.exists(path)\\n        # Add words to the dictionary\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            tokens = 0\\n            for line in f:\\n                words = line.split() + [\\'<eos>\\']\\n                tokens += len(words)\\n                for word in words:\\n                    self.dictionary.add_word(word)\\n\\n        # Tokenize file content\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            ids = torch.LongTensor(tokens)\\n            token = 0\\n            for line in f:\\n                words = line.split() + [\\'<eos>\\']\\n                for word in words:\\n                    ids[token] = self.dictionary.word2idx[word]\\n                    token += 1\\n\\n        return ids\\n\\nclass SentCorpus(object):\\n    def __init__(self, path):\\n        self.dictionary = Dictionary()\\n        self.train = self.tokenize(os.path.join(path, \\'train.txt\\'))\\n        self.valid = self.tokenize(os.path.join(path, \\'valid.txt\\'))\\n        self.test = self.tokenize(os.path.join(path, \\'test.txt\\'))\\n\\n    def tokenize(self, path):\\n        \"\"\"Tokenizes a text file.\"\"\"\\n        assert os.path.exists(path)\\n        # Add words to the dictionary\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            tokens = 0\\n            for line in f:\\n                words = line.split() + [\\'<eos>\\']\\n                tokens += len(words)\\n                for word in words:\\n                    self.dictionary.add_word(word)\\n\\n        # Tokenize file content\\n        sents = []\\n        with open(path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            for line in f:\\n                if not line:\\n                    continue\\n                words = line.split() + [\\'<eos>\\']\\n                sent = torch.LongTensor(len(words))\\n                for i, word in enumerate(words):\\n                    sent[i] = self.dictionary.word2idx[word]\\n                sents.append(sent)\\n\\n        return sents\\n\\nclass BatchSentLoader(object):\\n    def __init__(self, sents, batch_size, pad_id=0, cuda=False, volatile=False):\\n        self.sents = sents\\n        self.batch_size = batch_size\\n        self.sort_sents = sorted(sents, key=lambda x: x.size(0))\\n        self.cuda = cuda\\n        self.volatile = volatile\\n        self.pad_id = pad_id\\n\\n    def __next__(self):\\n        if self.idx >= len(self.sort_sents):\\n            raise StopIteration\\n\\n        batch_size = min(self.batch_size, len(self.sort_sents)-self.idx)\\n        batch = self.sort_sents[self.idx:self.idx+batch_size]\\n        max_len = max([s.size(0) for s in batch])\\n        tensor = torch.LongTensor(max_len, batch_size).fill_(self.pad_id)\\n        for i in range(len(batch)):\\n            s = batch[i]\\n            tensor[:s.size(0),i].copy_(s)\\n        if self.cuda:\\n            tensor = tensor.cuda()\\n\\n        self.idx += batch_size\\n\\n        return tensor\\n    \\n    next = __next__\\n\\n    def __iter__(self):\\n        self.idx = 0\\n        return self\\n\\nif __name__ == \\'__main__\\':\\n    corpus = SentCorpus(\\'../penn\\')\\n    loader = BatchSentLoader(corpus.test, 10)\\n    for i, d in enumerate(loader):\\n        print(i, d.size())\\n'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27210\n",
      "17\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"Given the following code information, for the following and access the code quality in terms of following points.\n",
    "    Answer the following with a score 0-10.\n",
    "        1. lines of code\n",
    "        2. cyclomatic complexity\n",
    "        3. nesting depth\n",
    "        4. code duplication\n",
    "        5. code coupling\n",
    "        6. Code readability\n",
    "        7. Code maintanibility\n",
    "        8. Proper documentation\n",
    "        9. Proper function doc strings\n",
    "        10. Proper maintained readme (1=yes or 0=no).\n",
    "\n",
    "    Code:\n",
    "        {text}\n",
    "    \"\"\"\n",
    "\n",
    "refine_template = \"\"\"Given the information about the code quality till now,\n",
    "{existing_answer}\n",
    "\n",
    "Analyse the code below on the parameters given below and update them\n",
    "1. lines of code\n",
    "2. cyclomatic complexity\n",
    "3. nesting depth\n",
    "4. code duplication\n",
    "5. code coupling\n",
    "6. Code readability\n",
    "7. Code maintanibility\n",
    "8. Proper documentation\n",
    "9. Proper function doc strings\n",
    "10. Proper maintained readme (yes or no).\n",
    "\n",
    "{text}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "reduce_template_string = \"\"\"Given the information about the code quality, \n",
    "Aggregate the results below and convert it to python dict\n",
    "    {text}\n",
    "    Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "# refine_prompt = PromptTemplate(\n",
    "#     input_variables=[\"existing_answer\", \"text\"],\n",
    "#     template=refine_template,\n",
    "# )\n",
    "reduce_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=reduce_template_string,\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    ")\n",
    "print(len(codebase))\n",
    "texts = text_splitter.split_text(codebase)\n",
    "docs = [Document(page_content=t) for t in texts]\n",
    "print(len(docs))\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    return_intermediate_steps=False,\n",
    "    map_prompt=prompt,\n",
    "    combine_prompt=reduce_prompt,\n",
    ")\n",
    "print(len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d681ff3d148ffba33420097363ef4c47 in your message.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 136 ms, sys: 30.7 ms, total: 167 ms\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "# %time output = await chain.acall({\"input_documents\": docs}, return_only_outputs=True)\n",
    "%time output = chain({\"input_documents\": docs}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intermediate_steps': ['1. lines of code: 36\\n2. cyclomatic complexity: 1\\n3. nesting depth: 1\\n4. code duplication: 0\\n5. code coupling: 0\\n6. Code readability: 8\\n7. Code maintainability: 8\\n8. Proper documentation: 1\\n9. Proper function doc strings: 0\\n10. Proper maintained readme (1=yes or 0=no): 1',\n",
       "  '1. Lines of code: 10\\n2. Cyclomatic complexity: 1\\n3. Nesting depth: 1\\n4. Code duplication: 0\\n5. Code coupling: 0\\n6. Code readability: 8\\n7. Code maintainability: 8\\n8. Proper documentation: 3\\n9. Proper function doc strings: 0\\n10. Proper maintained readme (1=yes or 0=no): 1',\n",
       "  '1. lines of code - 15 (excluding comments)\\n2. cyclomatic complexity - 1\\n3. nesting depth - 1\\n4. code duplication - 0\\n5. code coupling - 0\\n6. Code readability - 8\\n7. Code maintainability - 8\\n8. Proper documentation - 7\\n9. Proper function doc strings - 0\\n10. Proper maintained readme - 1',\n",
       "  '1. lines of code: 42\\n2. cyclomatic complexity: 1\\n3. nesting depth: 1\\n4. code duplication: 0\\n5. code coupling: 2\\n6. Code readability: 8\\n7. Code maintainability: 8\\n8. Proper documentation: 10\\n9. Proper function doc strings: 0\\n10. Proper maintained readme: 0',\n",
       "  '1. Lines of code: 35 (Score: 9)\\n2. Cyclomatic complexity: 2 (Score: 9)\\n3. Nesting depth: 3 (Score: 7)\\n4. Code duplication: 0 (Score: 10)\\n5. Code coupling: 2 (Score: 8)\\n6. Code readability: 8 (Score: 8)\\n7. Code maintainability: 8 (Score: 8)\\n8. Proper documentation: 0 (Score: 5)\\n9. Proper function doc strings: 0 (Score: 5)\\n10. Proper maintained readme: 0 (Score: 5)',\n",
       "  '1. lines of code - 8\\n2. cyclomatic complexity - 1\\n3. nesting depth - 2\\n4. code duplication - 0\\n5. code coupling - 2\\n6. Code readability - 8\\n7. Code maintainability - 8\\n8. Proper documentation - 6\\n9. Proper function doc strings - 6\\n10. Proper maintained readme (1=yes or 0=no) - 0',\n",
       "  '1. Lines of code: 33\\nScore: 8\\n\\n2. Cyclomatic complexity: 2\\nScore: 9\\n\\n3. Nesting depth: 2\\nScore: 9\\n\\n4. Code duplication: 0\\nScore: 10\\n\\n5. Code coupling: 3\\nScore: 7\\n\\n6. Code readability: 8\\nScore: 8\\n\\n7. Code maintainability: 8\\nScore: 8\\n\\n8. Proper documentation: 0\\nScore: 5\\n\\n9. Proper function doc strings: 0\\nScore: 5\\n\\n10. Proper maintained readme (1=yes or 0=no): 0\\nScore: 5',\n",
       "  '1. lines of code - 8\\n2. cyclomatic complexity - 1\\n3. nesting depth - 1\\n4. code duplication - 0\\n5. code coupling - 2\\n6. Code readability - 8\\n7. Code maintainability - 8\\n8. Proper documentation - 0\\n9. Proper function doc strings - 0\\n10. Proper maintained readme (1=yes or 0=no) - 0',\n",
       "  '1. lines of code: 38\\n2. cyclomatic complexity: 1\\n3. nesting depth: 2\\n4. code duplication: 0\\n5. code coupling: 0\\n6. Code readability: 8\\n7. Code maintainability: 8\\n8. Proper documentation: 0\\n9. Proper function doc strings: 0\\n10. Proper maintained readme: 0',\n",
       "  '1. lines of code - 10 (only 10 lines of code)\\n2. cyclomatic complexity - 1 (no branching or looping statements)\\n3. nesting depth - 1 (no nested blocks)\\n4. code duplication - 0 (no repeated code)\\n5. code coupling - 2 (some coupling between modules imported)\\n6. Code readability - 8 (code is easy to read and understand)\\n7. Code maintainability - 7 (code can be easily maintained)\\n8. Proper documentation - 0 (no documentation provided)\\n9. Proper function doc strings - 0 (no function doc strings provided)\\n10. Proper maintained readme - 0 (no readme provided)',\n",
       "  '1. Score: 4 - The code has 30 lines, which is not excessively long, but could still be improved by breaking down into smaller functions.\\n2. Score: 3 - There is no visible control flow statements, so the cyclomatic complexity is low.\\n3. Score: 1 - There is no visible nesting in the code, so the nesting depth is low.\\n4. Score: 9 - There does not seem to be any code duplication in this code.\\n5. Score: 6 - There is some code coupling between the arguments, as some of them depend on others. However, this is to be expected in a script that takes arguments.\\n6. Score: 8 - The code is fairly readable, with clear variable names and formatting. However, some comments could be added to explain the purpose of certain arguments.\\n7. Score: 7 - The code should be relatively easy to maintain, as it is not excessively long and has a clear purpose. However, there could be more modularity to make it easier to change specific parts of the code if needed.\\n8. Score: 3 - There is no visible documentation in the code, which could make it difficult for others to understand what the code does and how to use it.\\n9. Score: 2 - There are no visible function docstrings, which could make it difficult for others to understand what specific functions do.\\n10. Score: 0 - There is no visible readme file, which could make it difficult for others to understand how to use the code.',\n",
       "  '1. Lines of code: 10\\n2. Cyclomatic complexity: 1\\n3. Nesting depth: 1\\n4. Code duplication: 0\\n5. Code coupling: 1 (due to the use of argparse library)\\n6. Code readability: 7\\n7. Code maintainability: 8\\n8. Proper documentation: 6\\n9. Proper function doc strings: 0\\n10. Proper maintained readme: 0',\n",
       "  '1. lines of code: 24\\n2. cyclomatic complexity: 2\\n3. nesting depth: 3\\n4. code duplication: 0\\n5. code coupling: 1\\n6. Code readability: 8\\n7. Code maintainability: 7\\n8. Proper documentation: 2\\n9. Proper function doc strings: 0\\n10. Proper maintained readme: 0',\n",
       "  '1. lines of code: 41\\n2. cyclomatic complexity: 4\\n3. nesting depth: 2\\n4. code duplication: 0\\n5. code coupling: 1\\n6. Code readability: 8\\n7. Code maintainability: 8\\n8. Proper documentation: 6\\n9. Proper function doc strings: 5\\n10. Proper maintained readme (1=yes or 0=no): 0',\n",
       "  '1. Lines of code - 30\\n2. Cyclomatic complexity - 4\\n3. Nesting depth - 3\\n4. Code duplication - 0\\n5. Code coupling - 2\\n6. Code readability - 8\\n7. Code maintainability - 8\\n8. Proper documentation - 7\\n9. Proper function doc strings - 7\\n10. Proper maintained readme - 0',\n",
       "  '1. Lines of code: 28\\n2. Cyclomatic complexity: 3\\n3. Nesting depth: 2\\n4. Code duplication: 0\\n5. Code coupling: 1\\n6. Code readability: 8\\n7. Code maintainability: 8\\n8. Proper documentation: 6\\n9. Proper function doc strings: 6\\n10. Proper maintained readme: 0',\n",
       "  '1. lines of code: 7\\n2. cyclomatic complexity: 1\\n3. nesting depth: 1\\n4. code duplication: 0\\n5. code coupling: 1\\n6. Code readability: 8\\n7. Code maintainability: 8\\n8. Proper documentation: 2\\n9. Proper function doc strings: 2\\n10. Proper maintained readme: 0'],\n",
       " 'output_text': '1. Lines of code: 15\\n2. Cyclomatic complexity: 1\\n3. Nesting depth: 1\\n4. Code duplication: 0\\n5. Code coupling: 0\\n6. Code readability: 9\\n7. Code maintainability: 9\\n8. Proper documentation: 8\\n9. Proper function doc strings: 8\\n10. Proper maintained readme: 0'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"lines of code\": [46, 11, 40, 43, 23, 39, 34, 11, 30, 20, 38, 25, 25, 7],\n",
      "    \"cyclomatic complexity\": [1, 1, 1, 2, 1, 8, 1, null, 1, 2, 4, 5, 3, 1],\n",
      "    \"nesting depth\": [2, 1, 1, 2, 1, 1, 2, 1, 1, 3, 2, 3, null, 1],\n",
      "    \"code duplication\": [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "    \"code coupling\": [1, 0, 1, 2, 0, 6, 2, 2, 1, 3, 1, 3, 3, 0],\n",
      "    \"code readability\": [8, 8, 7, 8, 8, 7, 8, 8, 8, 8, 7, 8, 8, 8],\n",
      "    \"code maintainability\": [9, 8, 8, 9, 8, 8, 8, 8, 9, 7, 8, 7, 7, 8],\n",
      "    \"proper documentation\": [1, 1, 6, 5, 7, 6, 3, 7, 7, 2, 7, 6, 5, 5],\n",
      "    \"proper function doc strings\": [0, 0, 0, 0, 7, 0, 2, 0, 5, 1, 6, 5, 0, 5],\n",
      "    \"proper maintained readme\": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(output['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "directory_path = 'repos/darts/'\n",
    "paths = get_file_paths(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
